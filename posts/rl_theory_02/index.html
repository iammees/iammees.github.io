<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.290">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-04-22">

<title>iammees - Theory of RL II: Multi-Armed Bandits</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">iammees</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iammees" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Theory of RL II: Multi-Armed Bandits</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 22, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p><strong>Bandit problems</strong> are a special case of the RL problem in which there is <strong>only a single state</strong>. The name comes from slot machines which have a single lever (arm) and the very bad habit of stealing your money. You can imagine the multi-armed (also called k-armed) setting as follows: You are in a casino and face <span class="math inline">\(k\)</span> slot machines, each having an unknown probability of returning the reward. Your goal is to maximize your winnings in the long run by choosing the slot machine(s) with the highest reward probability.</p>
<p>Let’s look at multi-armed bandits in RL terms: At each time step <span class="math inline">\(t\)</span>, the agent has a choice among <span class="math inline">\(k\)</span> different actions. His reward <span class="math inline">\(R_t\)</span> is chosen from a stationary probability distribution that depends on the selected action <span class="math inline">\(A_t\)</span>. The expected reward (that is, the true <em>value</em>) of each action <span class="math inline">\(a\)</span>, <span class="math inline">\(q_*(a)\)</span>, is unknown to the agent:</p>
<p><span class="math display">\[q_*(a)=\mathbb{E} \left[R_t \vert A_t = a \right]\]</span></p>
<p>The agent’s estimate of <span class="math inline">\(q_*(a)\)</span> at time step <span class="math inline">\(t\)</span> is denoted <span class="math inline">\(Q_t(a)\)</span>. The goal of the agent is to maximize the cumulative reward <span class="math inline">\(\sum_{t=1}^{T} R_t\)</span> which is the same as to minimize the so-called <strong>regret</strong> (or loss), <span class="math inline">\(\mathcal{L_t}\)</span>, due to not picking the optimal action</p>
<p><span class="math display">\[\mathcal{L}_T = \mathbb{E} \left[ \sum_{t=1}^{T} \left(v_* - q_*(A_t)\right) \right],\]</span></p>
<p>where <span class="math inline">\(v_*\)</span> denotes the true value of the optimal action <span class="math inline">\(a_*\)</span>.</p>
<p>As we will see, this simplified version of the Markov decision process (MDP) is particularly suited to investigate the <strong>exploration vs.&nbsp;exploitation trade-off</strong> that arises in RL. Imagine you can choose between 10 slot machines and randomly select machine #5 to begin with. Say, you get the reward. Should you keep choosing machine #5? In RL, this is called <em>exploitation</em>: you choose the best option you know about. But since you’ve tried only one machine, there certainly could be another one that is better (that is, has a higher reward probability). To find out whether there in fact is, you must try other slot machines. This is called <em>exploration</em> since you explore other options to possibly find one that offers a higher reward probability than the ones you tried out before. Likely neither exploitation (only if your first choice is indeed the best) nor exploration (only if all choices are equally good) alone will suffice to maximize the cumulative reward. This is why agents must balance exploitation and exploration in order to reach their goal. Don’t forget that the stochasticity of the environment adds to the problem. While you got the reward when trying machine #5 for the first time, you may lose the second and third time you try it. This is why an agent needs a good strategy for reliably estimating the action-values by trial-and-error.</p>
<p>Note: A general intro into the topic can be found <a href="https://arxiv.org/abs/1904.07272">here</a>. For more on practical applications of multi-armed bandits, see <a href="https://arxiv.org/abs/1904.10040">here</a>.</p>
<p>Let’s put this in code. We’ll write a class <code>Bandit</code> that implements a multi-armed bandit with <span class="math inline">\(k\)</span> arms where each arm has a given probability of returning the reward (this is known as a Bernoulli bandit).</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> collections <span class="im">import</span> defaultdict</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">7</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="197">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Bandit:</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, k, probs<span class="op">=</span><span class="va">None</span>):</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.k <span class="op">=</span> k</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> probs <span class="kw">is</span> <span class="va">None</span>:</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.probs <span class="op">=</span> rng.random(<span class="va">self</span>.k)</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.probs <span class="op">=</span> probs</span>
<span id="cb3-8"><a href="#cb3-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-9"><a href="#cb3-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.best_prob <span class="op">=</span> np.<span class="bu">max</span>(<span class="va">self</span>.probs)</span>
<span id="cb3-10"><a href="#cb3-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.best_arm <span class="op">=</span> np.argmax(<span class="va">self</span>.probs)</span>
<span id="cb3-11"><a href="#cb3-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.sorted_arms <span class="op">=</span> np.argsort(<span class="va">self</span>.probs)</span>
<span id="cb3-12"><a href="#cb3-12" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-13"><a href="#cb3-13" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> gen_reward(<span class="va">self</span>, k):</span>
<span id="cb3-14"><a href="#cb3-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> rng.binomial(<span class="dv">1</span>, <span class="va">self</span>.probs[k])</span>
<span id="cb3-15"><a href="#cb3-15" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb3-16"><a href="#cb3-16" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot(<span class="va">self</span>, highlight_best<span class="op">=</span><span class="va">False</span>):</span>
<span id="cb3-17"><a href="#cb3-17" aria-hidden="true" tabindex="-1"></a>        _, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">4</span>, <span class="dv">3</span>))</span>
<span id="cb3-18"><a href="#cb3-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb3-19"><a href="#cb3-19" aria-hidden="true" tabindex="-1"></a>        ax.plot(np.arange(<span class="dv">0</span>, <span class="va">self</span>.k), <span class="va">self</span>.probs, <span class="st">"o"</span>, color<span class="op">=</span><span class="st">"black"</span>, ms<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb3-20"><a href="#cb3-20" aria-hidden="true" tabindex="-1"></a>        ax.grid(visible<span class="op">=</span><span class="va">True</span>, which<span class="op">=</span><span class="st">"both"</span>, lw<span class="op">=</span><span class="fl">0.3</span>)</span>
<span id="cb3-21"><a href="#cb3-21" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(ylim<span class="op">=</span>(<span class="dv">0</span>, <span class="dv">1</span>),</span>
<span id="cb3-22"><a href="#cb3-22" aria-hidden="true" tabindex="-1"></a>               xticks<span class="op">=</span>np.arange(<span class="va">self</span>.k),</span>
<span id="cb3-23"><a href="#cb3-23" aria-hidden="true" tabindex="-1"></a>               xlabel<span class="op">=</span><span class="st">"Arms"</span>,</span>
<span id="cb3-24"><a href="#cb3-24" aria-hidden="true" tabindex="-1"></a>               ylabel<span class="op">=</span><span class="st">"Reward probability"</span>)</span>
<span id="cb3-25"><a href="#cb3-25" aria-hidden="true" tabindex="-1"></a>        ax.tick_params(axis<span class="op">=</span><span class="st">"y"</span>, length<span class="op">=</span><span class="dv">0</span>)</span>
<span id="cb3-26"><a href="#cb3-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> spine <span class="kw">in</span> [<span class="st">"left"</span>, <span class="st">"right"</span>]:</span>
<span id="cb3-27"><a href="#cb3-27" aria-hidden="true" tabindex="-1"></a>            ax.spines[spine].set_visible(<span class="va">False</span>)</span>
<span id="cb3-28"><a href="#cb3-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> highlight_best:</span>
<span id="cb3-29"><a href="#cb3-29" aria-hidden="true" tabindex="-1"></a>            x, y <span class="op">=</span> <span class="va">self</span>.best_arm, <span class="va">self</span>.best_prob</span>
<span id="cb3-30"><a href="#cb3-30" aria-hidden="true" tabindex="-1"></a>            ax.plot(x, y, <span class="st">"o"</span>, color<span class="op">=</span><span class="st">"red"</span>, fillstyle<span class="op">=</span><span class="st">"none"</span>, ms<span class="op">=</span><span class="dv">10</span>, zorder<span class="op">=</span><span class="dv">2</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s look at our example bandit. The highlighted arm (#1 with the red circle) is the arm with the highest reward probability. Arms #5 and #7 come in second and third.</p>
<div class="cell" data-execution_count="198">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>bandit <span class="op">=</span> Bandit(<span class="dv">10</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>bandit.plot(highlight_best<span class="op">=</span><span class="va">True</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" class="quarto-discovered-preview-image img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="199">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a><span class="bu">list</span>(<span class="bu">zip</span>(bandit.sorted_arms, bandit.probs[bandit.sorted_arms]))[::<span class="op">-</span><span class="dv">1</span>]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="199">
<pre><code>[(1, 0.8972138009695755),
 (5, 0.8735534453962619),
 (7, 0.8212284183827663),
 (8, 0.7970694287520462),
 (2, 0.7756856902451935),
 (0, 0.625095466604667),
 (9, 0.4679349528437208),
 (4, 0.30016628491122543),
 (3, 0.22520718999059186),
 (6, 0.005265304565574724)]</code></pre>
</div>
</div>
<section id="action-value-methods" class="level2">
<h2 class="anchored" data-anchor-id="action-value-methods">Action-value Methods</h2>
<p>Unsurprisingly, methods for estimating the values of actions are called action-value methods. Since the true value of an action is the mean reward the agent gets when selecting this action, the most obvious way to estimate this is the <strong>sample-average method</strong> which just averages the actually observed rewards for each action:</p>
<p><span class="math display">\[Q_t(a) = \dfrac{\sum_{i=1}^{t-1} R_i \cdot \mathbb{1}_{A_i=a}}{\sum_{i=1}^{t-1} \mathbb{1}_{A_i=a}},\]</span></p>
<p>where <span class="math inline">\(\mathbb{1}\)</span> is a binary indicator function that is <span class="math inline">\(1\)</span> if the corresponding expression is true and <span class="math inline">\(0\)</span> otherwise. Put simply, we divide the sum of the rewards the agent has received so far for taking action <span class="math inline">\(a\)</span> by the number of times he selected action <span class="math inline">\(a\)</span>. By the law of large numbers <span class="math inline">\(Q_t(a)\)</span> converges to <span class="math inline">\(q_*(a)\)</span> as the denominator goes to infinity. To avoid division by zero (when the agent hasn’t chosen action <span class="math inline">\(a\)</span> yet), we need to define a default value for <span class="math inline">\(Q_t(a)\)</span>, say <span class="math inline">\(0\)</span> for now.</p>
<p>For a specific action <span class="math inline">\(a\)</span> this could be implemented like this:</p>
<p><span class="math display">\[Q_n = \dfrac{R_1 + R_2 + \dots + R_{n-1}}{n-1},\]</span></p>
<p>where <span class="math inline">\(Q_n\)</span> denotes the action-value estimate after the action has been selected <span class="math inline">\(n{-}1\)</span> times. The disadvantages of this implementation, however, are potentially huge requirements for memory (storing all the rewards) and computation (doing this calculation again and again). Thus, it is much more efficient to compute the estimates incrementally with a simple <strong>update rule</strong> that comes with constant computation per time step as well as constant memory (only for <span class="math inline">\(Q_n\)</span> and <span class="math inline">\(n\)</span>):</p>
<p><span class="math display">\[Q_{n+1} = Q_n + \dfrac{1}{n} \big[ R_n - Q_n \big]\]</span></p>
<p>Note that the pattern of this update rule</p>
<p><span class="math display">\[\text{NewEstimate} \leftarrow \text{OldEstimate} + \text{StepSize} \left[ \text{Target} - \text{OldEstimate} \right]\]</span></p>
<p>won’t occur here for the last time. To improve our estimate, we calculate the <strong>error</strong> <span class="math inline">\(\text{Target} - \text{OldEstimate}\)</span> and try to reduce it by taking a step towards the <strong>target</strong>. The size of that step is determined by the step-size parameter <span class="math inline">\(\alpha_t(a)\)</span>. In the simple algorithm presented above <span class="math inline">\(\alpha = \dfrac{1}{n}\)</span>.</p>
</section>
<section id="exploration-strategies" class="level2">
<h2 class="anchored" data-anchor-id="exploration-strategies">Exploration Strategies</h2>
<p>Now that we have a simple method for estimating action values, we can turn our attention to some classic exploration strategies. But first, let’s see why the pure exploitation and pure exploration strategies don’t work.</p>
<p>Always exploiting means always selecting the action with the highest estimated value (or one of the actions if there are multiple ones with the highest estimated value) and is called the <strong>greedy</strong> action selection method:</p>
<p><span class="math display">\[A_t = \text{argmax}_{a} Q_t(a)\]</span></p>
<p>Actions that appear to be inferior are never sampled under this strategy. Think back to the casino example one more time: Imagine you define the default values to be <span class="math inline">\(0\)</span> for every slot machine, randomly select machine #5 and get the reward. Under a greedy strategy, you will always choose machine #5 from now on since it has the highest action-value estimate. Only if the estimate falls below zero, you will try out another machine. Thus, if there are no negative rewards in the environment the greedy method will always stick to the first action. It is obvious why this doesn’t guarantee the best long-term reward.</p>
<p>The other extreme, pure random exploration, is not a useful strategy either. You will get very good action-value estimates in the long run by sampling every option, but don’t use your knowledge to maximize your rewards. This is why you need to balance the gathering of information (exploration) and the use of information (exploitation) to reach your goal.</p>
<p>Let’s see how this plays out with our example bandit. We define a <code>Strategy</code> class that allows us easily apply specific strategies to our problem.</p>
<div class="cell" data-execution_count="200">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>N_STEPS <span class="op">=</span> <span class="dv">1000</span></span>
<span id="cb7-2"><a href="#cb7-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-3"><a href="#cb7-3" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Strategy:</span>
<span id="cb7-4"><a href="#cb7-4" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span><span class="dv">1000</span>):</span>
<span id="cb7-5"><a href="#cb7-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.bandit <span class="op">=</span> bandit</span>
<span id="cb7-6"><a href="#cb7-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n_steps <span class="op">=</span> n_steps</span>
<span id="cb7-7"><a href="#cb7-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-8"><a href="#cb7-8" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.zeros(<span class="va">self</span>.bandit.k)</span>
<span id="cb7-9"><a href="#cb7-9" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> np.zeros(<span class="va">self</span>.bandit.k, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb7-10"><a href="#cb7-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-11"><a href="#cb7-11" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards <span class="op">=</span> np.empty(<span class="va">self</span>.n_steps, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb7-12"><a href="#cb7-12" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions <span class="op">=</span> np.empty(<span class="va">self</span>.n_steps, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb7-13"><a href="#cb7-13" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.regrets <span class="op">=</span> np.empty(<span class="va">self</span>.n_steps)</span>
<span id="cb7-14"><a href="#cb7-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-15"><a href="#cb7-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb7-16"><a href="#cb7-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">raise</span> <span class="pp">NotImplementedError</span></span>
<span id="cb7-17"><a href="#cb7-17" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-18"><a href="#cb7-18" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> update(<span class="va">self</span>, action, reward, t):</span>
<span id="cb7-19"><a href="#cb7-19" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n[action] <span class="op">+=</span> <span class="dv">1</span></span>
<span id="cb7-20"><a href="#cb7-20" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q[action] <span class="op">+=</span> (reward <span class="op">-</span> <span class="va">self</span>.Q[action]) <span class="op">/</span> <span class="va">self</span>.n[action]</span>
<span id="cb7-21"><a href="#cb7-21" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards[t] <span class="op">=</span> reward</span>
<span id="cb7-22"><a href="#cb7-22" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions[t] <span class="op">=</span> action</span>
<span id="cb7-23"><a href="#cb7-23" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.regrets[t] <span class="op">=</span> <span class="va">self</span>.bandit.best_prob <span class="op">-</span> <span class="va">self</span>.bandit.probs[action]</span>
<span id="cb7-24"><a href="#cb7-24" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-25"><a href="#cb7-25" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>):</span>
<span id="cb7-26"><a href="#cb7-26" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_steps):</span>
<span id="cb7-27"><a href="#cb7-27" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.step()</span>
<span id="cb7-28"><a href="#cb7-28" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="va">self</span>.bandit.gen_reward(action)</span>
<span id="cb7-29"><a href="#cb7-29" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.update(action, reward, t)</span>
<span id="cb7-30"><a href="#cb7-30" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-31"><a href="#cb7-31" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb7-32"><a href="#cb7-32" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.zeros(<span class="va">self</span>.bandit.k)</span>
<span id="cb7-33"><a href="#cb7-33" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> np.zeros(<span class="va">self</span>.bandit.k, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb7-34"><a href="#cb7-34" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.rewards <span class="op">=</span> np.empty(<span class="va">self</span>.n_steps, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb7-35"><a href="#cb7-35" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.actions <span class="op">=</span> np.empty(<span class="va">self</span>.n_steps, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb7-36"><a href="#cb7-36" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.regrets <span class="op">=</span> np.empty(<span class="va">self</span>.n_steps)</span>
<span id="cb7-37"><a href="#cb7-37" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb7-38"><a href="#cb7-38" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> plot_actions(<span class="va">self</span>):</span>
<span id="cb7-39"><a href="#cb7-39" aria-hidden="true" tabindex="-1"></a>        _, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb7-40"><a href="#cb7-40" aria-hidden="true" tabindex="-1"></a>        ax.scatter(np.arange(<span class="va">self</span>.n_steps), <span class="va">self</span>.actions, c<span class="op">=</span><span class="st">"black"</span>, alpha<span class="op">=</span><span class="fl">0.5</span>)</span>
<span id="cb7-41"><a href="#cb7-41" aria-hidden="true" tabindex="-1"></a>        ax.<span class="bu">set</span>(xticks<span class="op">=</span>np.linspace(<span class="dv">0</span>, <span class="va">self</span>.n_steps, <span class="dv">5</span>),</span>
<span id="cb7-42"><a href="#cb7-42" aria-hidden="true" tabindex="-1"></a>               yticks<span class="op">=</span>np.arange(<span class="va">self</span>.bandit.k),</span>
<span id="cb7-43"><a href="#cb7-43" aria-hidden="true" tabindex="-1"></a>               xlabel<span class="op">=</span><span class="st">"Steps"</span>,</span>
<span id="cb7-44"><a href="#cb7-44" aria-hidden="true" tabindex="-1"></a>               ylabel<span class="op">=</span><span class="st">"Arms"</span>)</span>
<span id="cb7-45"><a href="#cb7-45" aria-hidden="true" tabindex="-1"></a>        ax.grid()</span>
<span id="cb7-46"><a href="#cb7-46" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb7-47"><a href="#cb7-47" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> plot_actions(strategy):</span>
<span id="cb7-48"><a href="#cb7-48" aria-hidden="true" tabindex="-1"></a>    strategy.run()</span>
<span id="cb7-49"><a href="#cb7-49" aria-hidden="true" tabindex="-1"></a>    strategy.plot_actions()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="201">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> GreedyExploitation(Strategy):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS):</span>
<span id="cb8-3"><a href="#cb8-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(GreedyExploitation, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb8-4"><a href="#cb8-4" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-5"><a href="#cb8-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb8-6"><a href="#cb8-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"PureExploitation"</span></span>
<span id="cb8-7"><a href="#cb8-7" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb8-8"><a href="#cb8-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb8-9"><a href="#cb8-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> <span class="kw">not</span> np.<span class="bu">any</span>(<span class="va">self</span>.Q):</span>
<span id="cb8-10"><a href="#cb8-10" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> rng.integers(<span class="dv">0</span>, <span class="va">self</span>.bandit.k)</span>
<span id="cb8-11"><a href="#cb8-11" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb8-12"><a href="#cb8-12" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>As we can see below, the greedy strategy starts by picking a random action (arm #8 in this case). If, by chance, this arm returns the reward, the strategy will stay with this arm regardless of its reward probability. Otherwise the strategy would randomly choose arms until one returns the reward.</p>
<div class="cell" data-execution_count="202">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb9"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb9-1"><a href="#cb9-1" aria-hidden="true" tabindex="-1"></a>plot_actions(GreedyExploitation(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-9-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="203">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> RandomExploration(Strategy):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS):</span>
<span id="cb10-3"><a href="#cb10-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(RandomExploration, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb10-4"><a href="#cb10-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-5"><a href="#cb10-5" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb10-6"><a href="#cb10-6" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="st">"PureExploration"</span></span>
<span id="cb10-7"><a href="#cb10-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb10-8"><a href="#cb10-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb10-9"><a href="#cb10-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> rng.integers(<span class="dv">0</span>, <span class="va">self</span>.bandit.k)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The random exploration strategy is the other extreme; it continues to explore all possible actions.</p>
<div class="cell" data-execution_count="204">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>plot_actions(RandomExploration(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-11-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="epsilon-greedy-strategy" class="level3">
<h3 class="anchored" data-anchor-id="epsilon-greedy-strategy">Epsilon-greedy Strategy</h3>
<p>A simple improvement of the naive greedy approach is to exploit most of the time, but, with small probability <span class="math inline">\(\epsilon\)</span>, sometimes select an action randomly (i.e.&nbsp;independently of the action-value estimates), thereby exploring the other options. This strategy is called <strong><span class="math inline">\(\epsilon\)</span>-greedy</strong> and is guaranteed to converge to the true action-values, assuming that, in the limit, every action will be sampled an infinite number of times. <span class="math inline">\(\epsilon\)</span>-greedy turns out to work surprisingly well in many problems.</p>
<div class="cell" data-execution_count="205">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EpsilonGreedy(Strategy):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS, epsilon<span class="op">=</span><span class="fl">0.04</span>):</span>
<span id="cb12-3"><a href="#cb12-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(EpsilonGreedy, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb12-4"><a href="#cb12-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilon <span class="op">=</span> epsilon</span>
<span id="cb12-5"><a href="#cb12-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-6"><a href="#cb12-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb12-7"><a href="#cb12-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"EpsilonGreedy, epsilon=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>epsilon<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb12-8"><a href="#cb12-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb12-9"><a href="#cb12-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb12-10"><a href="#cb12-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rng.random() <span class="op">&lt;</span> <span class="va">self</span>.epsilon:</span>
<span id="cb12-11"><a href="#cb12-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> rng.integers(<span class="dv">0</span>, <span class="va">self</span>.bandit.k)</span>
<span id="cb12-12"><a href="#cb12-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb12-13"><a href="#cb12-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p><span class="math inline">\(\epsilon\)</span> is a hyperparameter that we have to choose depending on the problem. In the example below with <span class="math inline">\(\epsilon=0.04\)</span>, the epsilon-greedy strategy finds the best arm after around 600 steps.</p>
<div class="cell" data-execution_count="206">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>plot_actions(EpsilonGreedy(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="decaying-epsilon-greedy-strategy" class="level3">
<h3 class="anchored" data-anchor-id="decaying-epsilon-greedy-strategy">Decaying Epsilon-greedy Strategy</h3>
<p>Intuitively, an agent’s need for exploration is high in the beginning when he doesn’t have much information about the environment yet, but gets progressively smaller because he improves his value estimations as he gets more and more experience. This intuition can be implemented into the <span class="math inline">\(\epsilon\)</span>-greedy strategy by starting with a higher <span class="math inline">\(\epsilon\)</span> to emphasize exploration early on and decaying it going forward to focus more on exploitation. Note that there are plenty of ways to decay <span class="math inline">\(\epsilon\)</span>, from simple linear decaying to more sophisticated approaches.</p>
<div class="cell" data-execution_count="207">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EpsilonGreedyLinDecay(Strategy):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS, decay_ratio<span class="op">=</span><span class="fl">0.3</span>, start_eps<span class="op">=</span><span class="fl">1.0</span>, min_eps<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(EpsilonGreedyLinDecay, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decay_ratio <span class="op">=</span> decay_ratio</span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start_eps <span class="op">=</span> start_eps</span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_eps <span class="op">=</span> min_eps</span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"EpsilonGreedyLinDecay, decay_ratio=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>decay_ratio<span class="sc">}</span><span class="ss">, start_eps=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>start_eps<span class="sc">}</span><span class="ss">, min_eps=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>min_eps<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, t):</span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> t <span class="op">/</span> (<span class="va">self</span>.decay_ratio <span class="op">*</span> <span class="va">self</span>.n_steps)) <span class="op">*</span> (<span class="va">self</span>.start_eps <span class="op">-</span> <span class="va">self</span>.min_eps) <span class="op">+</span> <span class="va">self</span>.min_eps</span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>        epsilon <span class="op">=</span> np.clip(epsilon, <span class="va">self</span>.min_eps, <span class="va">self</span>.start_eps)</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rng.random() <span class="op">&lt;</span> epsilon:</span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> rng.integers(<span class="dv">0</span>, <span class="va">self</span>.bandit.k)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q)</span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>):</span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_steps):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.step(t)</span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="va">self</span>.bandit.gen_reward(action)</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.update(action, reward, t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The decay introduces more hyperparameters that we need to tune for our problem. With the default setting defined by the epsilon-greedy strategy with linear decay, we settle on the second-best arm:</p>
<div class="cell" data-execution_count="208">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>plot_actions(EpsilonGreedyLinDecay(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="209">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> EpsilonGreedyExpDecay(Strategy):</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span><span class="dv">1000</span>, decay_ratio<span class="op">=</span><span class="fl">0.3</span>, start_eps<span class="op">=</span><span class="fl">1.0</span>, min_eps<span class="op">=</span><span class="fl">0.01</span>):</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(EpsilonGreedyExpDecay, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb16-4"><a href="#cb16-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decay_ratio <span class="op">=</span> decay_ratio</span>
<span id="cb16-5"><a href="#cb16-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start_eps <span class="op">=</span> start_eps</span>
<span id="cb16-6"><a href="#cb16-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_eps <span class="op">=</span> min_eps</span>
<span id="cb16-7"><a href="#cb16-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-8"><a href="#cb16-8" aria-hidden="true" tabindex="-1"></a>        epsilons <span class="op">=</span> (<span class="fl">0.01</span> <span class="op">/</span> np.logspace(<span class="op">-</span><span class="dv">2</span>, <span class="dv">0</span>, <span class="bu">int</span>(<span class="va">self</span>.n_steps <span class="op">*</span> <span class="va">self</span>.decay_ratio)))</span>
<span id="cb16-9"><a href="#cb16-9" aria-hidden="true" tabindex="-1"></a>        epsilons <span class="op">=</span> epsilons <span class="op">*</span> (<span class="va">self</span>.start_eps <span class="op">-</span> <span class="va">self</span>.min_eps) <span class="op">+</span> <span class="va">self</span>.min_eps</span>
<span id="cb16-10"><a href="#cb16-10" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.epsilons <span class="op">=</span> np.pad(epsilons, (<span class="dv">0</span>, <span class="bu">int</span>((<span class="dv">1</span> <span class="op">-</span> <span class="va">self</span>.decay_ratio) <span class="op">*</span> <span class="va">self</span>.n_steps)), <span class="st">'edge'</span>)</span>
<span id="cb16-11"><a href="#cb16-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-12"><a href="#cb16-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb16-13"><a href="#cb16-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"EpsilonGreedyExpDecay, decay_ratio=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>decay_ratio<span class="sc">}</span><span class="ss">, start_eps=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>start_eps<span class="sc">}</span><span class="ss">, min_eps=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>min_eps<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb16-14"><a href="#cb16-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-15"><a href="#cb16-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, t):</span>
<span id="cb16-16"><a href="#cb16-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> rng.random() <span class="op">&lt;</span> <span class="va">self</span>.epsilons[t]:</span>
<span id="cb16-17"><a href="#cb16-17" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> rng.integers(<span class="dv">0</span>, <span class="va">self</span>.bandit.k)</span>
<span id="cb16-18"><a href="#cb16-18" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb16-19"><a href="#cb16-19" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q)</span>
<span id="cb16-20"><a href="#cb16-20" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb16-21"><a href="#cb16-21" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>):</span>
<span id="cb16-22"><a href="#cb16-22" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_steps):</span>
<span id="cb16-23"><a href="#cb16-23" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.step(t)</span>
<span id="cb16-24"><a href="#cb16-24" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="va">self</span>.bandit.gen_reward(action)</span>
<span id="cb16-25"><a href="#cb16-25" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.update(action, reward, t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The same strategy with exponential decay finds the best arm, however.</p>
<p>Naturally, chance plays a role here. To see which exploration strategy really works best we would need to rerun our experiment with many different seeds.</p>
<div class="cell" data-execution_count="210">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>plot_actions(EpsilonGreedyExpDecay(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="softmax-strategy" class="level3">
<h3 class="anchored" data-anchor-id="softmax-strategy">Softmax Strategy</h3>
<p>Depending on the problem, it can be a disadvantage of the <span class="math inline">\(\epsilon\)</span>-greedy strategy that it selects all actions with equal probability when exploring. The softmax strategy remedies this by making the probability of taking an action proportional to its current Q-value estimate. Put simply, actions with high value estimates are selected more frequently than actions with low estimates. This strategy is also called <strong>Boltzmann exploration</strong> since the agent <strong>draws actions from a Boltzmann (softmax) distribution over the action-value estimates</strong>. Note that the agent’s preference for actions with high estimates can be controlled by the <strong>temperature</strong> parameter <span class="math inline">\(\tau\)</span>: the agent selects the action with the highest value estimate as <span class="math inline">\(\tau\)</span> approaches <span class="math inline">\(0\)</span>, and samples an action uniformly as <span class="math inline">\(\tau\)</span> approaches infinity.</p>
<p><span class="math display">\[\pi(a) = \dfrac{\exp \left( \dfrac{Q(a)}{\tau} \right)}{\sum_{b=0}^{n} \exp \left( \dfrac{Q(b)}{\tau} \right)}\]</span></p>
<div class="cell" data-execution_count="211">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> Softmax(Strategy):</span>
<span id="cb18-2"><a href="#cb18-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS, decay_ratio<span class="op">=</span><span class="fl">0.05</span>, start_temp<span class="op">=</span><span class="dv">1000</span>, min_temp<span class="op">=</span><span class="fl">1e-10</span>):</span>
<span id="cb18-3"><a href="#cb18-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(Softmax, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb18-4"><a href="#cb18-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.decay_ratio <span class="op">=</span> decay_ratio</span>
<span id="cb18-5"><a href="#cb18-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.start_temp <span class="op">=</span> start_temp</span>
<span id="cb18-6"><a href="#cb18-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.min_temp <span class="op">=</span> min_temp</span>
<span id="cb18-7"><a href="#cb18-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-8"><a href="#cb18-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb18-9"><a href="#cb18-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"Softmax, decay_ratio=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>decay_ratio<span class="sc">}</span><span class="ss">, start_temp=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>start_temp<span class="sc">}</span><span class="ss">, min_temp=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>min_temp<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb18-10"><a href="#cb18-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-11"><a href="#cb18-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, t):</span>
<span id="cb18-12"><a href="#cb18-12" aria-hidden="true" tabindex="-1"></a>        temp <span class="op">=</span> (<span class="dv">1</span> <span class="op">-</span> t <span class="op">/</span> (<span class="va">self</span>.n_steps <span class="op">*</span> <span class="va">self</span>.decay_ratio)) <span class="op">*</span> (<span class="va">self</span>.start_temp <span class="op">-</span> <span class="va">self</span>.min_temp) <span class="op">+</span> <span class="va">self</span>.min_temp</span>
<span id="cb18-13"><a href="#cb18-13" aria-hidden="true" tabindex="-1"></a>        temp <span class="op">=</span> np.clip(temp, <span class="va">self</span>.min_temp, <span class="va">self</span>.start_temp)</span>
<span id="cb18-14"><a href="#cb18-14" aria-hidden="true" tabindex="-1"></a>        Q_scaled <span class="op">=</span> <span class="va">self</span>.Q <span class="op">/</span> temp</span>
<span id="cb18-15"><a href="#cb18-15" aria-hidden="true" tabindex="-1"></a>        Q_exp <span class="op">=</span> np.exp(Q_scaled <span class="op">-</span> np.<span class="bu">max</span>(Q_scaled))</span>
<span id="cb18-16"><a href="#cb18-16" aria-hidden="true" tabindex="-1"></a>        probs <span class="op">=</span> Q_exp <span class="op">/</span> np.<span class="bu">sum</span>(Q_exp)</span>
<span id="cb18-17"><a href="#cb18-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> rng.choice(np.arange(<span class="bu">len</span>(probs)), size<span class="op">=</span><span class="dv">1</span>, p<span class="op">=</span>probs).item()</span>
<span id="cb18-18"><a href="#cb18-18" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb18-19"><a href="#cb18-19" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>):</span>
<span id="cb18-20"><a href="#cb18-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_steps):</span>
<span id="cb18-21"><a href="#cb18-21" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.step(t)</span>
<span id="cb18-22"><a href="#cb18-22" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="va">self</span>.bandit.gen_reward(action)</span>
<span id="cb18-23"><a href="#cb18-23" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.update(action, reward, t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="212">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb19"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb19-1"><a href="#cb19-1" aria-hidden="true" tabindex="-1"></a>plot_actions(Softmax(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="optimistic-initialization" class="level3">
<h3 class="anchored" data-anchor-id="optimistic-initialization">Optimistic Initialization</h3>
<p>Until now, we simply used <span class="math inline">\(0\)</span> as the default value for actions that are still unexplored, thereby injecting an initial bias into our method. We can use this bias as a simple way to improve the greedy approach by setting <strong>optimistic initial values</strong>. What does that mean? If we set the initial estimate to a high value, the agent will be “disappointed” by the rewards early on which encourages exploration until the estimates improve and start converging to the true values. You can see below that this can work quite well in the bandit example. Optimistic initial values have several drawbacks, though. They assume that we have some knowledge about what a “high” value in the environment is (if we set it too high, the algorithm needs many time steps to get more realistic estimates; if we set it too low, the strategy isn’t optimistic and doesn’t work as intended) and they don’t work in nonstationary environments since they only encourage exploration in the beginning.</p>
<div class="cell" data-execution_count="213">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> OptimisticInitialization(Strategy):</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS, init_val<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(OptimisticInitialization, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.init_val <span class="op">=</span> init_val</span>
<span id="cb20-5"><a href="#cb20-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.full(<span class="va">self</span>.bandit.k, <span class="va">self</span>.init_val)</span>
<span id="cb20-6"><a href="#cb20-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> np.ones(<span class="va">self</span>.bandit.k, dtype<span class="op">=</span><span class="st">"int"</span>)</span>
<span id="cb20-7"><a href="#cb20-7" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-8"><a href="#cb20-8" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb20-9"><a href="#cb20-9" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"OptimisticInitialization, init_val=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>init_val<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb20-10"><a href="#cb20-10" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-11"><a href="#cb20-11" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb20-12"><a href="#cb20-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q)</span>
<span id="cb20-13"><a href="#cb20-13" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb20-14"><a href="#cb20-14" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb20-15"><a href="#cb20-15" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.Q <span class="op">=</span> np.full(<span class="va">self</span>.bandit.k, <span class="va">self</span>.init_val)</span>
<span id="cb20-16"><a href="#cb20-16" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.n <span class="op">=</span> np.ones(<span class="va">self</span>.bandit.k, dtype<span class="op">=</span><span class="st">"int"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="214">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb21"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb21-1"><a href="#cb21-1" aria-hidden="true" tabindex="-1"></a>plot_actions(OptimisticInitialization(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="upper-confidence-bound-ucb-action-selection" class="level3">
<h3 class="anchored" data-anchor-id="upper-confidence-bound-ucb-action-selection">Upper-Confidence-Bound (UCB) Action Selection</h3>
<p>Another approach that follows the principle of “optimism in the face of uncertainty” is the UCB strategy which accounts for both the estimated action-value <em>and</em> the uncertainty of the estimate by greedily selecting the action that maximizes the upper confidence bound. To implement this method, we add an uncertainty bonus <span class="math inline">\(U(a)\)</span> to the Q-value estimates and always select the action with the highest total value. The idea is that the true action-value is below that upper confidence bound with high probability. Put simply, we select the action with the highest <em>potential</em> to be optimal. Let’s look at the math:</p>
<p><span class="math display">\[A_t = \text{argmax}_a \left[ Q_t(a) + U_t{a}\right] = \text{argmax}_a \left[ Q_t(a) + c \sqrt{\dfrac{\ln{t}}{N_t(a)}}\right],\]</span></p>
<p>where <span class="math inline">\(\ln t\)</span> denotes the natural logarithm of <span class="math inline">\(t\)</span>, <span class="math inline">\(N_t(a)\)</span> denotes the number of times the agent selected action <span class="math inline">\(a\)</span> prior to time step <span class="math inline">\(t\)</span>, and <span class="math inline">\(c\)</span> is a hyperparameter that controls the size of the bonus (and hence the agent’s preference for exploration). As a result, an action that has been selected only a few times after a number of trials has higher associated uncertainty (and higher need for exploration) than an action that was sampled often. As the agent gets more experience and thus more confident in his estimates, the effect of the uncertainty term fades.</p>
<div class="cell" data-execution_count="215">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> UCB(Strategy):</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS, c<span class="op">=</span><span class="fl">0.2</span>):</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(UCB, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb22-4"><a href="#cb22-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.c <span class="op">=</span> c</span>
<span id="cb22-5"><a href="#cb22-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-6"><a href="#cb22-6" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb22-7"><a href="#cb22-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"UCB, c=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>c<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb22-8"><a href="#cb22-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-9"><a href="#cb22-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>, t):</span>
<span id="cb22-10"><a href="#cb22-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> t <span class="op">&lt;</span> <span class="va">self</span>.bandit.k:</span>
<span id="cb22-11"><a href="#cb22-11" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> t</span>
<span id="cb22-12"><a href="#cb22-12" aria-hidden="true" tabindex="-1"></a>        <span class="cf">else</span>:</span>
<span id="cb22-13"><a href="#cb22-13" aria-hidden="true" tabindex="-1"></a>            <span class="cf">return</span> np.argmax(<span class="va">self</span>.Q <span class="op">+</span> <span class="va">self</span>.c <span class="op">*</span> np.sqrt(np.log(t) <span class="op">/</span> <span class="va">self</span>.n))</span>
<span id="cb22-14"><a href="#cb22-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb22-15"><a href="#cb22-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>):</span>
<span id="cb22-16"><a href="#cb22-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_steps):</span>
<span id="cb22-17"><a href="#cb22-17" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.step(t)</span>
<span id="cb22-18"><a href="#cb22-18" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="va">self</span>.bandit.gen_reward(action)</span>
<span id="cb22-19"><a href="#cb22-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.update(action, reward, t)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="216">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb23"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb23-1"><a href="#cb23-1" aria-hidden="true" tabindex="-1"></a>plot_actions(UCB(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-23-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="thompson-sampling" class="level3">
<h3 class="anchored" data-anchor-id="thompson-sampling">Thompson Sampling</h3>
<p>While the UCB strategy is deterministic, Thompson sampling is a sample-based approach for selecting actions that essentially comes down to Bayesian inference. For the Bernoulli bandit, we assume that each Q-value estimate <span class="math inline">\(Q(a)\)</span> follows a Beta distribution, <span class="math inline">\(\text{Beta}(\alpha, \beta)\)</span>, which is parameterized by two positive parameters <span class="math inline">\(\alpha\)</span> and <span class="math inline">\(\beta\)</span> that correspond to the number of times we got the reward versus the number of times we didn’t when we took the action. Initially, we set the parameters based on our beliefs for every action, e.g.&nbsp;<span class="math inline">\(\alpha=1\)</span> and <span class="math inline">\(\beta=1\)</span> when we believe (without confidence) that the reward probability is <span class="math inline">\(0.5\)</span>. Then, at each time step <span class="math inline">\(t\)</span>, we sample an expected reward for every action from the respective prior distribution <span class="math inline">\(\text{Beta}(\alpha_i, \beta_i)\)</span> and simply select the best available action. After we have observed the true reward, we update the action’s Beta distribution accordingly.</p>
<div class="cell" data-execution_count="217">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb24"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb24-1"><a href="#cb24-1" aria-hidden="true" tabindex="-1"></a><span class="kw">class</span> ThompsonSampling(Strategy):</span>
<span id="cb24-2"><a href="#cb24-2" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__init__</span>(<span class="va">self</span>, bandit, n_steps<span class="op">=</span>N_STEPS, alpha<span class="op">=</span><span class="dv">1</span>, beta<span class="op">=</span><span class="dv">1</span>):</span>
<span id="cb24-3"><a href="#cb24-3" aria-hidden="true" tabindex="-1"></a>        <span class="bu">super</span>(ThompsonSampling, <span class="va">self</span>).<span class="fu">__init__</span>(bandit, n_steps)</span>
<span id="cb24-4"><a href="#cb24-4" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alpha <span class="op">=</span> alpha</span>
<span id="cb24-5"><a href="#cb24-5" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.beta <span class="op">=</span> beta</span>
<span id="cb24-6"><a href="#cb24-6" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> np.full(<span class="va">self</span>.bandit.k, <span class="va">self</span>.alpha)</span>
<span id="cb24-7"><a href="#cb24-7" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> np.full(<span class="va">self</span>.bandit.k, <span class="va">self</span>.beta)</span>
<span id="cb24-8"><a href="#cb24-8" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-9"><a href="#cb24-9" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> <span class="fu">__repr__</span>(<span class="va">self</span>):</span>
<span id="cb24-10"><a href="#cb24-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> <span class="ss">f"ThompsonSampling, alpha=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>alpha<span class="sc">}</span><span class="ss">, beta=</span><span class="sc">{</span><span class="va">self</span><span class="sc">.</span>beta<span class="sc">}</span><span class="ss">"</span></span>
<span id="cb24-11"><a href="#cb24-11" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-12"><a href="#cb24-12" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> step(<span class="va">self</span>):</span>
<span id="cb24-13"><a href="#cb24-13" aria-hidden="true" tabindex="-1"></a>        <span class="cf">return</span> np.argmax(rng.beta(<span class="va">self</span>.alphas, <span class="va">self</span>.betas))</span>
<span id="cb24-14"><a href="#cb24-14" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-15"><a href="#cb24-15" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> run(<span class="va">self</span>):</span>
<span id="cb24-16"><a href="#cb24-16" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> t <span class="kw">in</span> <span class="bu">range</span>(<span class="va">self</span>.n_steps):</span>
<span id="cb24-17"><a href="#cb24-17" aria-hidden="true" tabindex="-1"></a>            action <span class="op">=</span> <span class="va">self</span>.step()</span>
<span id="cb24-18"><a href="#cb24-18" aria-hidden="true" tabindex="-1"></a>            reward <span class="op">=</span> <span class="va">self</span>.bandit.gen_reward(action)</span>
<span id="cb24-19"><a href="#cb24-19" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.alphas[action] <span class="op">+=</span> reward</span>
<span id="cb24-20"><a href="#cb24-20" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.betas[action] <span class="op">+=</span> (<span class="dv">1</span> <span class="op">-</span> reward)</span>
<span id="cb24-21"><a href="#cb24-21" aria-hidden="true" tabindex="-1"></a>            <span class="va">self</span>.update(action, reward, t)</span>
<span id="cb24-22"><a href="#cb24-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb24-23"><a href="#cb24-23" aria-hidden="true" tabindex="-1"></a>    <span class="kw">def</span> reset(<span class="va">self</span>):</span>
<span id="cb24-24"><a href="#cb24-24" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.alphas <span class="op">=</span> np.full(<span class="va">self</span>.bandit.k, <span class="va">self</span>.alpha)</span>
<span id="cb24-25"><a href="#cb24-25" aria-hidden="true" tabindex="-1"></a>        <span class="va">self</span>.betas <span class="op">=</span> np.full(<span class="va">self</span>.bandit.k, <span class="va">self</span>.beta)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="218">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb25"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb25-1"><a href="#cb25-1" aria-hidden="true" tabindex="-1"></a>plot_actions(ThompsonSampling(bandit))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-25-output-1.png" class="img-fluid"></p>
</div>
</div>
</section>
<section id="a-quick-comparison" class="level3">
<h3 class="anchored" data-anchor-id="a-quick-comparison">A quick comparison</h3>
<p>Finally, we can do a quick comparison using data from 20 runs:</p>
<div class="cell" data-execution_count="265">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> experiment(k<span class="op">=</span><span class="dv">10</span>, n_steps<span class="op">=</span><span class="dv">1000</span>, runs<span class="op">=</span><span class="dv">20</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    regrets <span class="op">=</span> defaultdict(<span class="bu">list</span>)</span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> run <span class="kw">in</span> <span class="bu">range</span>(runs):</span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a>        bandit <span class="op">=</span> Bandit(k)</span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> strategy <span class="kw">in</span> [GreedyExploitation(bandit, n_steps<span class="op">=</span>n_steps),</span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a>                         RandomExploration(bandit, n_steps<span class="op">=</span>n_steps),</span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># EpsilonGreedy(bandit, n_steps=n_steps, epsilon=0.01),</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a>                         EpsilonGreedy(bandit, n_steps<span class="op">=</span>n_steps, epsilon<span class="op">=</span><span class="fl">0.025</span>),</span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># EpsilonGreedy(bandit, n_steps=n_steps, epsilon=0.04),</span></span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>                         EpsilonGreedyLinDecay(bandit, n_steps<span class="op">=</span>n_steps, decay_ratio<span class="op">=</span><span class="fl">0.3</span>, start_eps<span class="op">=</span><span class="fl">1.0</span>, min_eps<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># EpsilonGreedyLinDecay(bandit, n_steps=n_steps, decay_ratio=0.25, start_eps=1.0, min_eps=0.01),</span></span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># EpsilonGreedyLinDecay(bandit, n_steps=n_steps, decay_ratio=0.2, start_eps=1.0, min_eps=0.01),</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># EpsilonGreedyExpDecay(bandit, n_steps=n_steps, decay_ratio=0.3, start_eps=1.0, min_eps=0.01),</span></span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>                         EpsilonGreedyExpDecay(bandit, n_steps<span class="op">=</span>n_steps, decay_ratio<span class="op">=</span><span class="fl">0.25</span>, start_eps<span class="op">=</span><span class="fl">1.0</span>, min_eps<span class="op">=</span><span class="fl">0.01</span>),</span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># EpsilonGreedyExpDecay(bandit, n_steps=n_steps, decay_ratio=0.2, start_eps=1.0, min_eps=0.01),</span></span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># Softmax(bandit, n_steps=n_steps, decay_ratio=0.075, start_temp=1000, min_temp=1e-10),</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>                         Softmax(bandit, n_steps<span class="op">=</span>n_steps, decay_ratio<span class="op">=</span><span class="fl">0.05</span>, start_temp<span class="op">=</span><span class="dv">1000</span>, min_temp<span class="op">=</span><span class="fl">1e-10</span>),</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># Softmax(bandit, n_steps=n_steps, decay_ratio=0.03, start_temp=1000, min_temp=1e-10),</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># UCB(bandit, n_steps=n_steps, c=0.3),</span></span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>                         <span class="co"># UCB(bandit, n_steps=n_steps, c=0.2),</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>                         UCB(bandit, n_steps<span class="op">=</span>n_steps, c<span class="op">=</span><span class="fl">0.1</span>),</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>                         ThompsonSampling(bandit, n_steps<span class="op">=</span>n_steps)]:</span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>            strategy.run()</span>
<span id="cb26-27"><a href="#cb26-27" aria-hidden="true" tabindex="-1"></a>            regrets[<span class="bu">str</span>(strategy)].append(strategy.regrets)</span>
<span id="cb26-28"><a href="#cb26-28" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb26-29"><a href="#cb26-29" aria-hidden="true" tabindex="-1"></a>    fig, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">10</span>, <span class="dv">5</span>))</span>
<span id="cb26-30"><a href="#cb26-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> strategy, result <span class="kw">in</span> regrets.items():</span>
<span id="cb26-31"><a href="#cb26-31" aria-hidden="true" tabindex="-1"></a>        ax.plot(np.arange(n_steps), np.cumsum(np.mean(result, axis<span class="op">=</span><span class="dv">0</span>)), label<span class="op">=</span>strategy)</span>
<span id="cb26-32"><a href="#cb26-32" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb26-33"><a href="#cb26-33" aria-hidden="true" tabindex="-1"></a>    ax.legend()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="266">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>experiment()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-27-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In the next post, we’ll look at the bigger picture again and see how we can solve the RL problem with dynamic programming.</p>


</section>
</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>