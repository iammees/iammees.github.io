<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.290">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2022-11-17">

<title>iammees - Tackling Tabular Problems IV: Models &amp; Hyperparameter Optimization</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">iammees</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iammees" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Tackling Tabular Problems IV: Models &amp; Hyperparameter Optimization</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">November 17, 2022</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<section id="models" class="level2">
<h2 class="anchored" data-anchor-id="models">Models</h2>
<p>Like with previous aspects of the pipeline, there are no precise rules for picking the right model for a particular problem because the choice depends on a multitude of different factors. However, there are some rules of thumb (given that we are optimizing for the predictive power of our model):</p>
<ul>
<li>For smaller datasets (with several hundred to several thousand observations), linear models (e.g., ridge regression, logistic regression) generally work best. Random Forests or Support Vector Machines are also worth a try. Note that Nvidia’s <a href="https://github.com/rapidsai/cuml"><code>cuml</code></a> library provides GPU acceleration for all these models.</li>
<li>For larger datasets (from several thousand to hundreds of millions of observations), GBDTs are the gold standard. Their rule-based approach is ideally suited for the typical irregular patterns of tabular data, and with a large enough dataset overfitting can be prevented. Also, GBDTs (especially XGBoost) nowadays profit heavily from GPU acceleration. If the data comes from physical processes or simulations, however, neural networks will often shine since they are biased towards smooth solutions. This also holds when interpolation is very important (e.g., because the samples are quite far from each other) because the piecewise constant functions learned by tree-based models are not suited for interpolation. Furthermore, neural networks are the way to go for representation learning. Finally, if we need the best possible predictive performance, deep learning usually works well as a complement to tree-based models in ensembles. Have a look at this recent <a href="https://arxiv.org/pdf/2207.08815.pdf">paper</a> for more on tree-based models and neural networks for tabular data.</li>
<li>For huge datasets (with, say, more than a billion observations), neural networks are typically the best choice.</li>
</ul>
</section>
<section id="gbdt-libraries-and-their-hyperparameters" class="level2">
<h2 class="anchored" data-anchor-id="gbdt-libraries-and-their-hyperparameters">GBDT libraries and their hyperparameters</h2>
<p>For the sake of brevity, we’ll only dive deeper into GBDTs and their hyperparameters. We’ll look at the most common implementations: XGBoost, LightGBM, CatBoost, and scikit-learn’s HistGradientBoosting.</p>
<p>Note:</p>
<ul>
<li>Knowing how GBDTs work is a prequisite for understanding their hyperparameters; see <a href="https://explained.ai/gradient-boosting/">here</a> for a thorough explanation.</li>
<li>The tips below come from my own experience (especially XGBoost and LightGBM) as well as from other people’s code and what I found online (CatBoost, HistGradientBoosting).</li>
</ul>
<section id="xgboost" class="level3">
<h3 class="anchored" data-anchor-id="xgboost">XGBoost</h3>
<p><a href="https://github.com/dmlc/xgboost">XGBoost</a> was the <a href="https://arxiv.org/pdf/1603.02754.pdf">first library</a> that provided a highly optimized implementation of GBDTs. Nowadays, it is primarily developed by Nvidia leading to built-in GPU acceleration which massively speeds up model training.</p>
<p>Let’s have a look at the most important hyperparameters:</p>
<ul>
<li><code>n_estimators</code>: This is the number of trees fitted (which is equal to the number of boosting iterations). Generally a higher number of trees improves the model’s performance (at least up to some point), but also increases the risk of overfitting. In general, around <code>1,000</code> trees should be enough; using many more is not needed in most problem settings (<code>5,000</code> should definitely be enough). Important: <code>n_estimators</code> has to be tuned in conjunction with the <code>learning_rate</code>. Thus, a good approach can be to set <code>n_estimators=1000</code> and try out different learning rates (or do a grid search) before proceeding with tuning other hyperparameters. After finding a good hyperparameter setting, we can still tune both the number of trees and the learning rate. (Note that this can be done the other way around, too. That is, we can set the learning rate so that around 1,000 trees are used, and continue from there.)</li>
<li><code>learning_rate</code>: A learning rate between <code>0.001</code> and <code>0.1</code> should work fine. Generally, a lower learning rate should be combined with a higher number of trees.</li>
<li><code>max_depth</code>: Apart from <code>n_estimators</code> and <code>learning_rate</code>, <code>max_depth</code> is probably the most important hyperparameter. It determines the maximum depth of the trees; with a higher <code>max_depth</code> the model becomes more complex but also more likely to overfit. When tuning by hand, this is the first hyperparameter (after the number of trees/learning rate) that should be tuned. It is also the first parameter to decrease if overfitting is a problem. In general, a value between <code>3</code> and <code>10</code> should be fine. Depending on the problem, it can make sense to go a bit higher. Note that a higher <code>max_depth</code> results in slower computation and higher memory usage.</li>
<li><code>min_child_weight</code>: Technically, <code>min_child_weight</code> is the minimum sum of instance weight needed in a child. In lay terms it regularizes the model by limiting the depth of the trees. Generally, a higher <code>min_child_weight</code> means a more conservative model. A good range for tuning is between <code>1</code> (the default value) and <code>300</code>.</li>
<li><code>colsample_bytree</code>: This is the subsample ratio of columns when constructing each tree. Subsampling columns when constructing each tree can make the training faster and more robust to noise. Typically, the best value is between <code>0.5</code> and <code>0.9</code> (or even <code>0.3</code> and <code>1.0</code>).</li>
<li><code>subsample</code>: This is the subsample ratio of the training instances (subsampling occurs once in every boosting iteration). Like <code>colsample_bytree</code>, using <code>subsample</code> helps prevent overfitting by introducing randomness in the training process. Typically, the best value is between <code>0.5</code> and <code>0.8</code> (or even <code>0.4</code> and <code>1.0</code>). <code>0.7</code> is often a good starting point. Making use of <code>colsample_bytree</code> and <code>subsample</code> leads to more different tree splits.</li>
<li><code>reg_alpha</code>, <code>reg_lambda</code>: While <code>alpha</code> represents the L1 regularization term on weights, <code>lambda</code> is the L2 regularization term. Increasing these hyperparameters translates to making the model more conservative. There are two things to note: First, adding regularization is not always better. Second, for GBDTs it tends to be better to set <code>alpha</code> to (near) zero, while using a higher <code>lambda</code>. A good range for tuning is between <code>0.001</code> and <code>10.0</code>.</li>
<li><code>max_bin</code>: The maximum number of bins to bucket numerical features. The default is <code>256</code>. Can be increased to build a more complex model, but comes with a higher risk of overfitting and slows down training.</li>
<li><code>gamma</code>: <code>gamma</code> represents the minimum loss reduction required to make a further partition on a leaf node of the tree. That is, gamma controls the complexity of the model and can be increased to combat overfitting. Since a good value for gamma is highly dependent on the data set and the other hyperparameters, it is difficult to give a good range for tuning. As per this <a href="https://medium.com/data-design/xgboost-hi-im-gamma-what-can-i-do-for-you-and-the-tuning-of-regularization-a42ea17e6ab6">post</a>, it can help to increase <code>gamma</code> (the default is <code>0</code>; try <code>3</code>, <code>5</code>, or <code>10</code>, for example) if overfitting appears to be a huge problem. Values around 20 should only be used when the model uses a high <code>max_depth</code>.</li>
</ul>
<p>Apart from <code>n_estimators</code> and <code>learning_rate</code>, it is typically enough to tune <code>max_depth</code>, <code>min_child_weight</code>, <code>colsample_bytree</code>, <code>subsample</code>, and <code>reg_alpha</code>/<code>reg_lambda</code>.</p>
<p>Notes:</p>
<ul>
<li>The XGBoost library provides two APIs: its original “Learning API” and a scikit-learn wrapper interface, the “Scikit-Learn API”. Furthermore, XGBoost uses the custom data structure <code>DMatrix</code> to improve training speed and memory efficiency. For improved memory efficiency when training on the GPU the XGBoost library also provides the <code>DeviceQuantileDMatrix</code>.</li>
<li>Set <code>objective</code> to define the objective function and <code>eval_metric</code> for the evaluation metric. XGBoost provides <a href="https://xgboost.readthedocs.io/en/stable/parameter.html#learning-task-parameters">many options</a> for regression, classification, and ranking tasks. See <a href="https://xgboost.readthedocs.io/en/stable/tutorials/custom_metric_obj.html">here</a> for defining custom objectives or evaluation metrics.</li>
<li>Specify the <code>tree_method</code> as <code>gpu_hist</code> to use GPU acceleration. XGBoost supports fully distributed GPU training <a href="https://xgboost.readthedocs.io/en/stable/tutorials/dask.html">using Dask</a>.</li>
<li>Specify the <code>predictor</code> parameter to <code>cpu_predictor</code> to predict on the CPU or to <code>gpu_predictor</code> for using GPU accelerated prediction. Use <code>gpu_predictor</code> to compute SHAP values using GPU acceleration.</li>
<li>When there are unbalanced classes, use <code>scale_pos_weight</code> to control the balance of positive and negative weights. A typical value is <code>sum(negative instances) / sum(positive instances)</code>. See <a href="https://xgboost.readthedocs.io/en/stable/tutorials/param_tuning.html#handle-imbalanced-dataset">here</a> for more tips and <a href="https://github.com/dmlc/xgboost/blob/master/demo/kaggle-higgs/higgs-cv.py">here</a> for an example.</li>
<li>Depending on the problem, we may know that a given feature should generally have a positive or negative effect on the target (e.g., due to business constraints or scientific knowledge) or we may want to restrict the interaction of different features. XGBoost provides tutorials on using <a href="https://xgboost.readthedocs.io/en/latest/tutorials/monotonic.html">monotonic constraints</a> and <a href="https://xgboost.readthedocs.io/en/latest/tutorials/feature_interaction_constraint.html">feature interaction constraints</a>.</li>
<li>There is <a href="https://xgboost.readthedocs.io/en/stable/tutorials/multioutput.html">experimental support</a> for multi-output regression.</li>
<li>The integration with Apache Spark and Dask allows to handle huge volumes of data.</li>
<li>There is a <a href="https://docs.wandb.ai/guides/integrations/xgboost">callback</a> for Weights &amp; Biases.</li>
</ul>
</section>
<section id="lightgbm" class="level3">
<h3 class="anchored" data-anchor-id="lightgbm">LightGBM</h3>
<p><a href="https://github.com/microsoft/LightGBM">LightGBM</a>, developed by Microsoft, is an alternative to XGBoost. While XGBoost uses a level-wise strategy to grow trees (i.e., breadth-first search), LightGBM uses a leaf-wise growth strategy (i.e., depth-first search). This generally allows LightGBM to fit more quickly than XGBoost (at least when compared on CPUs).</p>
<ul>
<li><code>n_estimators</code>: In LightGBM the optimal range for the number of trees appears to be somewhat wider than in XGBoost. Try values between 100 and 10,000. Increase the <code>learning_rate</code> when you decrease <code>n_estimators</code>. Note that choosing the right values for the number of trees and the learning rate is highly dependent on the data and the objective.</li>
<li><code>learning_rate</code>: Try values between <code>0.001</code> and <code>0.1</code>. Use a small learning rate combined with a higher number of iterations to improve the accuracy of the model. Decrease the learning rate to combat overfitting.</li>
<li><code>max_depth</code>: Try values between <code>3</code> and <code>20</code>.</li>
<li><code>num_leaves</code>: The maximum number of final leaves for each tree and the main parameter to control the complexity of a LightGBM model. Can be between <code>2</code> and <span class="math inline">\(2^{\text{max depth}}\)</span>.</li>
<li><code>min_data_in_leaf</code>: The minimum number of data points in one leaf. Very important parameter to prevent overfitting in a leaf-wise tree (like LightGBM). Its optimal value depends on the number of training samples and <code>num_leaves</code>. Setting it to a large value can avoid growing too deep trees, but may cause underfitting. In practice, setting it to hundreds or thousands is enough for a large dataset.</li>
<li><code>max_bin</code>: The maximum of bins that each feature will be bucketed into. The default is <code>255</code>. Increasing <code>max_bin</code> up to, say, <code>400</code> allows more complexity but also increases the risk of overfitting. Decreasing <code>max_bin</code> can be used to speed up training.</li>
<li><code>colsample_bytree</code>: Try values between <code>0.3</code> and <code>1.0</code>.</li>
<li><code>subsample</code>: Try values between <code>0.1</code> and <code>1.0</code>.</li>
<li><code>subsample_freq</code>: The frequency (in terms of boosting iterations) at which LightGBM will subsample the training examples. If subsampling helps, try values between <code>1</code> and <code>10</code>.</li>
<li><code>reg_alpha</code>, <code>reg_lambda</code>: These are the same as in XGBoost.</li>
<li><code>min_gain_to_split</code>: Increase <code>min_gain_to_split</code> to combat overfitting. From the LightGBM docs: “When adding a new tree node, LightGBM chooses the split point that has the largest gain. Gain is basically the reduction in training loss that results from adding a split point. By default, LightGBM sets <code>min_gain_to_split</code> to <code>0.0</code>, which means ‘there is no improvement that is too small’. However, in practice you might find that very small improvements in the training loss don’t have a meaningful impact on the generalization error of the model.”</li>
</ul>
<p>Apart from <code>n_estimators</code> and <code>learning_rate</code>, it is typically enough to tune <code>num_leaves</code>, <code>min_data_in_leaf</code>, <code>colsample_bytree</code>, <code>subsample</code>, and <code>reg_alpha</code>/<code>reg_lambda</code>.</p>
<p>Notes:</p>
<ul>
<li>LightGBM provides two main APIs: the original “Training API” and a scikit-learn wrapper interface, the “Scikit-Learn API”.</li>
<li>Use the <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.Dataset.html"><code>Dataset</code> class</a> to handle data with LightGBM.</li>
<li>It is <a href="https://lightgbm.readthedocs.io/en/latest/GPU-Tutorial.html">possible</a> to train LightGBM using GPUs. XGBoost, however, generally provides (much) better GPU acceleration.</li>
<li>Set <code>objective</code> to define the objective function. See <a href="https://lightgbm.readthedocs.io/en/latest/Parameters.html#objective">here</a> for the many options provided by LightGBM.</li>
<li>Use <code>categorical_feature</code> to specify categorical features.</li>
<li>In binary classification, use <code>is_unbalance</code> or <code>scale_pos_weight</code> to handle class imbalances. In multi-class classification tasks, use <code>class_weight</code>. Per the LightGBM <a href="https://lightgbm.readthedocs.io/en/latest/pythonapi/lightgbm.LGBMClassifier.html">docs</a>, note that it may be necessary to <a href="https://scikit-learn.org/stable/modules/calibration.html">calibrate</a> the resulting class probabilities.</li>
<li>Since version 3.2.0 there is an <a href="https://lightgbm.readthedocs.io/en/latest/Python-API.html#training-api">integration with Dask</a>.</li>
<li>Use <code>monotone_constraints</code> to set monotonic constraints.</li>
<li>There is a <a href="https://docs.wandb.ai/guides/integrations/lightgbm">callback</a> for Weights &amp; Biases.</li>
</ul>
</section>
<section id="catboost" class="level3">
<h3 class="anchored" data-anchor-id="catboost">CatBoost</h3>
<p><a href="https://github.com/catboost/catboost">CatBoost</a> was originally developed by Yandex. As hinted by its name, CatBoost generally shines in datasets with many categorical features (CatBoost cleverly uses one-hot encoding and target encoding for this purpose). It can work really well with all kinds of datasets, however.</p>
<p>The eight most important hyperparameters:</p>
<ul>
<li><code>n_estimators</code>: Try values between <code>100</code> and <code>1,000</code>. CatBoost’s default is <code>1,000</code> trees; again, using many more typically isn’t necessary.</li>
<li><code>learning_rate</code>: Try values between <code>0.001</code> and <code>0.1</code>. Use a small learning rate combined with a higher number of iterations to improve the accuracy of the model. Decrease the learning rate to combat overfitting.</li>
<li><code>max_depth</code>: Per the <a href="https://catboost.ai/en/docs/concepts/parameter-tuning">CatBoost docs</a>, the optimal depth ranges from <code>4</code> to <code>10</code> and values from <code>6</code> to <code>10</code> are recommended. CatBoost doesn’t support values higher than <code>16</code>.</li>
<li><code>subsample</code>: Try values between <code>0.1</code> and <code>1.0</code>.</li>
<li><code>bagging_temperature</code>: Defines the settings of the Bayesian bootstrap which is used to assign random weights to objects. The default is <code>1.0</code>. A higher value translates to more aggressive bagging. If set to <code>0.0</code>, all weights are equal to <code>1</code>.</li>
<li><code>random_strength</code>: Per the CatBoost docs, this parameter is used when selecting splits: “On every iteration each possible split gets a score (for example, the score indicates how much adding this split will improve the loss function for the training dataset). The split with the highest score is selected. The scores have no randomness. A normally distributed random variable is added to the score of the feature. It has a zero mean and a variance that decreases during the training. The value of this parameter is the multiplier of the variance.” The default value is <code>1.0</code>. Try values between <code>1e-9</code> and <code>10.0</code>.</li>
<li><code>border_count</code>: The number of splits for numerical features. <code>128</code> should usually be enough. Set <code>border_count</code> to the default of <code>254</code> when training on the GPU to get the best possible performance. Larger values are generally not recommended and slow down training.</li>
<li><code>reg_lambda</code>: Try values between <code>2</code> and <code>30</code>.</li>
</ul>
<p>Apart from <code>n_estimators</code> and <code>learning_rate</code>, it is typically enough to tune <code>max_depth</code>, <code>subsample</code>, <code>reg_lambda</code>, <code>random_strength</code>, and <code>bagging_temperature</code>.</p>
<p>Notes:</p>
<ul>
<li>The CatBoost API is <a href="https://catboost.ai/en/docs/concepts/python-quickstart">compatible</a> with scikit-learn.</li>
<li>CatBoost models can be trained on the GPU (<code>task_type="GPU"</code>). XGBoost, however, generally provides (much) better GPU acceleration.</li>
<li>See <a href="https://catboost.ai/en/docs/concepts/loss-functions">here</a> for more on objective functions and metrics in CatBoost.</li>
<li>Use <code>cat_features</code> to specify categorical features.</li>
<li>Use <code>class_weights</code> to specify class weights in classification problems.</li>
</ul>
</section>
<section id="histgradientboosting" class="level3">
<h3 class="anchored" data-anchor-id="histgradientboosting">HistGradientBoosting</h3>
<p>Inspired by LightGBM, scikit-learn has added its own versions of GBDTs: <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingRegressor.html"><code>HistGradientBoostingRegressor</code></a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.ensemble.HistGradientBoostingClassifier.html"><code>HistGradientBoostingClassifier</code></a>. They have less hyperparameters than the other implementations and are well worth a try.</p>
<ul>
<li><code>max_iter</code>: This is the maximum number of trees fitted (i.e., equivalent to <code>n_estimators</code> in other libraries). The default value is 100. Try values between 100 and 10,000.</li>
<li><code>learning_rate</code>: Try values between <code>0.001</code> and <code>0.1</code>.</li>
<li><code>max_depth</code>: Try values between <code>3</code> and <code>12</code> (the ideal value is often around <code>6</code>). Note that the depth isn’t constrained by default.</li>
<li><code>max_leaf_nodes</code>: The maximum number of leaves for each tree. The default is <code>31</code>. Depending on the dataset, try values in a larger range (e.g., between <code>2</code> and <code>500</code>).</li>
<li><code>min_samples_leaf</code>: The minimum number of samples per leaf. The default is <code>20</code>. Depending on the dataset, try values between <code>2</code> and <code>300</code>.</li>
<li><code>max_bins</code>: The default is <code>255</code> (one more bin is always reserved for missing values). Using less bins can help prevent overfitting, but isn’t generally recommended by scikit-learn.</li>
<li><code>l2_regularization</code>: Comparable to <code>reg_lambda</code> in other libraries. The default is <code>0</code>. Try values up to <code>100</code>.</li>
</ul>
<p>Notes:</p>
<ul>
<li>Use the <code>loss</code> parameter to set the loss function.</li>
<li>Scikit-learn provides an example for using <a href="https://scikit-learn.org/stable/auto_examples/ensemble/plot_monotonic_constraints.html#sphx-glr-auto-examples-ensemble-plot-monotonic-constraints-py">monotonic constraints</a> (set the <code>monotonic_cst</code> parameter). Setting <a href="https://scikit-learn.org/stable/modules/ensemble.html#interaction-constraints">interaction constraints</a> is possible too (set the <code>interaction_cst</code> parameter).</li>
<li>Use the <code>categorical_features</code> parameter to indicate categorical features.</li>
<li>By default, early stopping is performed if there are at least 10,000 samples. We can control this behavior using the <code>early_stopping</code> parameter.</li>
</ul>
</section>
</section>
<section id="hyperparameter-optimization" class="level2">
<h2 class="anchored" data-anchor-id="hyperparameter-optimization">Hyperparameter Optimization</h2>
<p>Finding the right set of hyperparameters for a given problem can be very challenging. The automatic hyperparameter optimization methods that are briefly outlined below can generally be applied to any problems or models, not just tabular ones. Note: Before we can optimize hyperparameters, we of course need a model, a hyperparameter search space (i.e., a range of values for each hyperparameter), and a reliable CV scheme with an evaluation metric that we care about.</p>
<ul>
<li><strong>Grid search</strong>: Grid search translates to searching exhaustively through all possible hyperparameter combinations. The algorithm is highly parallelizable, but naturally suffers from the curse of dimensionality and is therefore not feasible in a high-dimensional search space. We can use it in the rare cases that don’t require a lot of tuning, though. Grid search is implemented by scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.GridSearchCV.html#sklearn.model_selection.GridSearchCV"><code>GridSearchCV</code></a>.</li>
<li><strong>Random search</strong>: Instead of trying every combinations, random search randomly samples the search space. Since this is generally feasible in high-dimensional spaces, doesn’t waste time with testing slightly different combinations, and still has a good chance of finding a good set of hyperparameters, random search is quite common in practice. It is implemented in scikit-learn’s <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV"><code>RandomizedSearchCV</code></a>.</li>
<li><strong>Halving search</strong>: Both grid search and random search cannot recognize ineffective hyperparameters or search intervals. To remedy this, scikit-learn also provides hyperparameter optimization based on <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.RandomizedSearchCV.html#sklearn.model_selection.RandomizedSearchCV">successive halving</a> (SH). In the first iteration, the SH algorithm evaluates all parameter combinations with a small amount of resources (often the number of training samples). Then, it selects only the best candidates for the next iteration which in turn uses more resources for evaluation. This process continues in the following iterations (i.e., only a subset of candidates proceeds to the next round and the corresponding performance estimates get more precise since more resources are allocated for evaluation). Scikit-learn implements the SH algorithm in <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingGridSearchCV.html#sklearn.model_selection.HalvingGridSearchCV"><code>HalvingGridSearchCV</code></a> and <a href="https://scikit-learn.org/stable/modules/generated/sklearn.model_selection.HalvingRandomSearchCV.html#sklearn.model_selection.HalvingRandomSearchCV"><code>HalvingRandomSearchCV</code></a>.</li>
<li><strong>Bayesian optimization</strong>: Bayesian optimization (BO) is a more sophisticated approach. Unlike grid search or random search, BO optimizes a surrogate function instead of the true objective function (because the true objective function may be expensive to evaluate, noisy, and evaluation doen’t give gradient information). In the beginning BO focuses on exploration to train the surrogate function, and then starts to exploit its approximate knowledge of the objective to sample more useful examples (i.e., hyperparameter combinations). For smooth search spaces, BO is typically based on Gaussian processes (with <a href="https://github.com/scikit-optimize/scikit-optimize">scikit-optimize</a> being the most prominent example). An alternative for more complex search spaces are Tree Parzen Estimators (TPEs) which sample parameter combinations from a multivariate probability distribution. TPE is the default sampler used by <a href="https://github.com/optuna/optuna">Optuna</a>, a widely used tool for hyperparameter optimization. See <a href="https://distill.pub/2020/bayesian-optimization/">here</a> for a nice and more in-depth explanation of Bayesian optimization.</li>
</ul>
<p>Finally, some general tips for GBDT hyperparameter tuning:</p>
<ul>
<li>Focus on key hyperparameters. It isn’t necessary to tune all available hyperparameters; instead focus on key hyperparameters that can be tuned intuitively.</li>
<li>Stay conservative. Using huge search spaces for finding the best possible hyperparameter configuration usually results in overfitting to the validation data. Thus, blindly trusting the result of an Optuna study is generally not a good idea (we need to at least check the best trials for consistency).</li>
<li>Don’t tune the hyperparameters too often. After the hyperparameters of a model have been tuned, the results cannot be reliably compared to older runs. Tune the hyperparameters of the model when the feature engineering is complete (and maybe once when a useful basic set of features has been created).</li>
<li>Don’t use early stopping. Instead of using a different number of trees for each fold (inevitably leading to overfitting), select the number of trees that works best across folds. We generally wouldn’t use different hyperparameters for our folds; this applies to the number of trees too. Also, using early stopping complicates retraining on the full dataset.</li>
<li>With some experience, tuning GBDTs manually can be a quite intuitive method for hyperparameter optimization (and get better results in less trials than Optuna). An example for XGBoost: Set <code>n_estimators=1000</code> and choose an adequate learning rate. Then, try some reasonable values for <code>max_depth</code> and plot the corresponding CV scores (we should be able to see whether we need to try more values). Proceed like this with <code>min_child_weight</code>, <code>colsample_bytree</code>, <code>subsample</code>, and <code>reg_alpha</code>/<code>reg_lambda</code>. This way, we’ll gather a better understanding of the role of each hyperparameter. In the end, see whether changes to the number of trees and/or the learning rate can lead to further improvements.</li>
</ul>
</section>
<section id="example-rocket-league" class="level2">
<h2 class="anchored" data-anchor-id="example-rocket-league">Example: Rocket League</h2>
<p>We’re ready to finalize our Rocket League example. Let’s tune the hyperparameters of the XGBoost model developed in the last post and see whether we can improve our CV score even further.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:02:15.882718Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:02:15.882399Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:02:21.150732Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:02:21.149432Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:02:15.882672Z&quot;}" data-trusted="true" data-execution_count="1">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> pandas <span class="im">as</span> pd</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cudf</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> cupy</span>
<span id="cb1-4"><a href="#cb1-4" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-5"><a href="#cb1-5" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> xgboost <span class="im">as</span> xgb</span>
<span id="cb1-6"><a href="#cb1-6" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> matplotlib.pyplot <span class="im">as</span> plt</span>
<span id="cb1-7"><a href="#cb1-7" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.model_selection <span class="im">import</span> GroupKFold</span>
<span id="cb1-8"><a href="#cb1-8" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> sklearn.metrics <span class="im">import</span> log_loss</span>
<span id="cb1-9"><a href="#cb1-9" aria-hidden="true" tabindex="-1"></a><span class="im">from</span> pathlib <span class="im">import</span> Path</span>
<span id="cb1-10"><a href="#cb1-10" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> re</span>
<span id="cb1-11"><a href="#cb1-11" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gc</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>data_path <span class="op">=</span> Path(<span class="st">"/kaggle/input/rocket-league-ds"</span>)</span>
<span id="cb2-2"><a href="#cb2-2" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-3"><a href="#cb2-3" aria-hidden="true" tabindex="-1"></a>train_df <span class="op">=</span> cudf.read_feather(data_path<span class="op">/</span><span class="st">"train_fe.feather"</span>)</span>
<span id="cb2-4"><a href="#cb2-4" aria-hidden="true" tabindex="-1"></a>test_df <span class="op">=</span> cudf.read_feather(data_path<span class="op">/</span><span class="st">"test_fe.feather"</span>)</span>
<span id="cb2-5"><a href="#cb2-5" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb2-6"><a href="#cb2-6" aria-hidden="true" tabindex="-1"></a>target <span class="op">=</span> <span class="st">"team_A_scoring_within_10sec"</span></span>
<span id="cb2-7"><a href="#cb2-7" aria-hidden="true" tabindex="-1"></a>features <span class="op">=</span> [col <span class="cf">for</span> col <span class="kw">in</span> train_df <span class="cf">if</span> col <span class="op">!=</span> target]</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:02:42.335561Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:02:42.335114Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:02:42.351015Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:02:42.348909Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:02:42.335515Z&quot;}" data-trusted="true" data-execution_count="8">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>xgb_params <span class="op">=</span> { </span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>    <span class="st">"eval_metric"</span>:<span class="st">"logloss"</span>,</span>
<span id="cb3-3"><a href="#cb3-3" aria-hidden="true" tabindex="-1"></a>    <span class="st">"objective"</span>:<span class="st">"binary:logistic"</span>,</span>
<span id="cb3-4"><a href="#cb3-4" aria-hidden="true" tabindex="-1"></a>    <span class="st">"tree_method"</span>:<span class="st">"gpu_hist"</span>,</span>
<span id="cb3-5"><a href="#cb3-5" aria-hidden="true" tabindex="-1"></a>    <span class="st">"predictor"</span>:<span class="st">"gpu_predictor"</span>,</span>
<span id="cb3-6"><a href="#cb3-6" aria-hidden="true" tabindex="-1"></a>    <span class="st">"random_state"</span>:<span class="dv">1</span>,</span>
<span id="cb3-7"><a href="#cb3-7" aria-hidden="true" tabindex="-1"></a>}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>The <code>optimize</code> function, which we’ll use for tuning the hyperparameters by hand, is pretty simple. It takes the name of a parameter and the corresponding search space, and it outputs the CV score for each parameter value as well as a plot showing all experiments.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:02:42.355699Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:02:42.353320Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:02:42.411598Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:02:42.409687Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:02:42.355635Z&quot;}" data-trusted="true" data-execution_count="9">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> optimize(df, param, search_space):</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>    cv <span class="op">=</span> GroupKFold(n_splits<span class="op">=</span><span class="dv">5</span>)</span>
<span id="cb4-3"><a href="#cb4-3" aria-hidden="true" tabindex="-1"></a>    oof <span class="op">=</span> np.zeros(df.shape[<span class="dv">0</span>])</span>
<span id="cb4-4"><a href="#cb4-4" aria-hidden="true" tabindex="-1"></a>    run_scores <span class="op">=</span> []</span>
<span id="cb4-5"><a href="#cb4-5" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-6"><a href="#cb4-6" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> run, val <span class="kw">in</span> <span class="bu">enumerate</span>(search_space):</span>
<span id="cb4-7"><a href="#cb4-7" aria-hidden="true" tabindex="-1"></a>        xgb_params[param] <span class="op">=</span> val</span>
<span id="cb4-8"><a href="#cb4-8" aria-hidden="true" tabindex="-1"></a>        fold_scores <span class="op">=</span> []</span>
<span id="cb4-9"><a href="#cb4-9" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-10"><a href="#cb4-10" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> fold, (train_idx, valid_idx) <span class="kw">in</span> <span class="bu">enumerate</span>(cv.split(df, groups<span class="op">=</span>df.game_num.to_numpy())):        </span>
<span id="cb4-11"><a href="#cb4-11" aria-hidden="true" tabindex="-1"></a>            dtrain <span class="op">=</span> xgb.DMatrix(data<span class="op">=</span>df.iloc[train_idx][features], label<span class="op">=</span>df.iloc[train_idx][target])</span>
<span id="cb4-12"><a href="#cb4-12" aria-hidden="true" tabindex="-1"></a>            dvalid <span class="op">=</span> xgb.DMatrix(data<span class="op">=</span>df.iloc[valid_idx][features], label<span class="op">=</span>df.iloc[valid_idx][target])</span>
<span id="cb4-13"><a href="#cb4-13" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-14"><a href="#cb4-14" aria-hidden="true" tabindex="-1"></a>            model <span class="op">=</span> xgb.train(xgb_params,</span>
<span id="cb4-15"><a href="#cb4-15" aria-hidden="true" tabindex="-1"></a>                              dtrain<span class="op">=</span>dtrain,</span>
<span id="cb4-16"><a href="#cb4-16" aria-hidden="true" tabindex="-1"></a>                              evals<span class="op">=</span>[(dtrain, <span class="st">"train"</span>), (dvalid, <span class="st">"valid"</span>)],</span>
<span id="cb4-17"><a href="#cb4-17" aria-hidden="true" tabindex="-1"></a>                              num_boost_round<span class="op">=</span><span class="dv">1000</span>,</span>
<span id="cb4-18"><a href="#cb4-18" aria-hidden="true" tabindex="-1"></a>                              verbose_eval<span class="op">=</span><span class="va">False</span>)</span>
<span id="cb4-19"><a href="#cb4-19" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-20"><a href="#cb4-20" aria-hidden="true" tabindex="-1"></a>            oof_preds <span class="op">=</span> model.predict(dvalid)</span>
<span id="cb4-21"><a href="#cb4-21" aria-hidden="true" tabindex="-1"></a>            oof[valid_idx] <span class="op">=</span> oof_preds</span>
<span id="cb4-22"><a href="#cb4-22" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb4-23"><a href="#cb4-23" aria-hidden="true" tabindex="-1"></a>            fold_score <span class="op">=</span> log_loss(df.iloc[valid_idx][target].to_numpy(), oof_preds)</span>
<span id="cb4-24"><a href="#cb4-24" aria-hidden="true" tabindex="-1"></a>            fold_scores.append(fold_score)</span>
<span id="cb4-25"><a href="#cb4-25" aria-hidden="true" tabindex="-1"></a>            </span>
<span id="cb4-26"><a href="#cb4-26" aria-hidden="true" tabindex="-1"></a>        run_score <span class="op">=</span> np.mean(fold_scores)</span>
<span id="cb4-27"><a href="#cb4-27" aria-hidden="true" tabindex="-1"></a>        run_scores.append(run_score)</span>
<span id="cb4-28"><a href="#cb4-28" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"Run </span><span class="sc">{</span>run<span class="sc">}</span><span class="ss"> with </span><span class="sc">{</span>param<span class="sc">}</span><span class="ss">=</span><span class="sc">{</span>val<span class="sc">:.4f}</span><span class="ss"> finished with score: </span><span class="sc">{</span>run_score<span class="sc">:.5f}</span><span class="ss">"</span>)</span>
<span id="cb4-29"><a href="#cb4-29" aria-hidden="true" tabindex="-1"></a>        </span>
<span id="cb4-30"><a href="#cb4-30" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"Best run: </span><span class="sc">{</span>param<span class="sc">}</span><span class="ss">=</span><span class="sc">{</span>search_space[np.argmin(run_scores)]<span class="sc">}</span><span class="ss"> with score </span><span class="sc">{</span>np<span class="sc">.</span><span class="bu">min</span>(run_scores)<span class="sc">}</span><span class="ss">"</span>)</span>
<span id="cb4-31"><a href="#cb4-31" aria-hidden="true" tabindex="-1"></a>    </span>
<span id="cb4-32"><a href="#cb4-32" aria-hidden="true" tabindex="-1"></a>    _, ax <span class="op">=</span> plt.subplots(figsize<span class="op">=</span>(<span class="dv">6</span>, <span class="dv">3</span>))</span>
<span id="cb4-33"><a href="#cb4-33" aria-hidden="true" tabindex="-1"></a>    ax.scatter(search_space, run_scores)</span>
<span id="cb4-34"><a href="#cb4-34" aria-hidden="true" tabindex="-1"></a>    ax.<span class="bu">set</span>(xlabel<span class="op">=</span>param, ylabel<span class="op">=</span><span class="st">"CV score"</span>)<span class="op">;</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s see how this works in practice. Having set the number of trees (<code>num_boost_round</code>) to <code>1000</code>, we begin by trying different values for the learning rate. Plotting the results helps us determine the optimal value for this parameter.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:02:42.420297Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:02:42.417015Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:21:17.082387Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:21:17.080592Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:02:42.420250Z&quot;}" data-trusted="true" data-execution_count="10">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>optimize(train_df, <span class="st">"learning_rate"</span>, np.logspace(np.log10(<span class="fl">0.001</span>), np.log10(<span class="fl">0.1</span>), <span class="dv">7</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Run 0 with learning_rate=0.0010 finished with score: 0.31120
Run 1 with learning_rate=0.0022 finished with score: 0.21784
Run 2 with learning_rate=0.0046 finished with score: 0.19517
Run 3 with learning_rate=0.0100 finished with score: 0.19302
Run 4 with learning_rate=0.0215 finished with score: 0.19224
Run 5 with learning_rate=0.0464 finished with score: 0.19221
Run 6 with learning_rate=0.1000 finished with score: 0.19370
Best run: learning_rate=0.046415888336127774 with score 0.19221204071717607</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-2.png" class="quarto-discovered-preview-image img-fluid"></p>
</div>
</div>
<p>We set the parameter to <code>0.045</code> and from now on treat it as fixed:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:21:17.084403Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:21:17.084011Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:21:17.089418Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:21:17.088185Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:21:17.084364Z&quot;}" data-trusted="true" data-execution_count="11">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb7"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb7-1"><a href="#cb7-1" aria-hidden="true" tabindex="-1"></a>xgb_params[<span class="st">"learning_rate"</span>] <span class="op">=</span> <span class="fl">0.045</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Now we proceed in the same way with the other hyperparameters that we want to tune.</p>
<p>Note: This may seem like a bad approach due to the complex dependencies between different hyperparameters which are often said to require an extensive grid search for tuning. In my experience though, manual tuning is an intuitive and, even more importantly, competitive approach to hyperparameter optimization that doesn’t end in bad surprises (that is, bad test scores) and improves our understanding of key hyperparameters along the way.</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:21:17.091697Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:21:17.090985Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:35:04.085897Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:35:04.084165Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:21:17.091606Z&quot;}" data-trusted="true" data-execution_count="12">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a>optimize(train_df, <span class="st">"max_depth"</span>, <span class="bu">range</span>(<span class="dv">3</span>, <span class="dv">9</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Run 0 with max_depth=3.0000 finished with score: 0.19328
Run 1 with max_depth=4.0000 finished with score: 0.19248
Run 2 with max_depth=5.0000 finished with score: 0.19219
Run 3 with max_depth=6.0000 finished with score: 0.19224
Run 4 with max_depth=7.0000 finished with score: 0.19251
Run 5 with max_depth=8.0000 finished with score: 0.19311
Best run: max_depth=5 with score 0.19218715537697664</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-8-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:35:04.088983Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:35:04.087952Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T10:35:04.096178Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T10:35:04.093347Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:35:04.088940Z&quot;}" data-trusted="true" data-execution_count="13">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a>xgb_params[<span class="st">"max_depth"</span>] <span class="op">=</span> <span class="dv">5</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T10:45:36.946315Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T10:45:36.945411Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T11:01:16.848439Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T11:01:16.846538Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T10:45:36.946261Z&quot;}" data-trusted="true" data-execution_count="15">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb11"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb11-1"><a href="#cb11-1" aria-hidden="true" tabindex="-1"></a>optimize(train_df, <span class="st">"colsample_bytree"</span>, np.arange(<span class="fl">0.3</span>, <span class="fl">1.1</span>, <span class="fl">0.1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Run 0 with colsample_bytree=0.3000 finished with score: 0.19274
Run 1 with colsample_bytree=0.4000 finished with score: 0.19251
Run 2 with colsample_bytree=0.5000 finished with score: 0.19239
Run 3 with colsample_bytree=0.6000 finished with score: 0.19228
Run 4 with colsample_bytree=0.7000 finished with score: 0.19222
Run 5 with colsample_bytree=0.8000 finished with score: 0.19221
Run 6 with colsample_bytree=0.9000 finished with score: 0.19216
Run 7 with colsample_bytree=1.0000 finished with score: 0.19217
Best run: colsample_bytree=0.9000000000000001 with score 0.19216419980671837</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-10-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T11:01:35.189089Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T11:01:35.188383Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T11:01:35.193698Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T11:01:35.192594Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T11:01:35.189051Z&quot;}" data-trusted="true" data-execution_count="16">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb13"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb13-1"><a href="#cb13-1" aria-hidden="true" tabindex="-1"></a>xgb_params[<span class="st">"colsample_bytree"</span>] <span class="op">=</span> <span class="fl">0.9</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T11:01:36.564281Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T11:01:36.563752Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T11:09:51.956192Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T11:09:51.955096Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T11:01:36.564244Z&quot;}" data-trusted="true" data-execution_count="17">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a>optimize(train_df, <span class="st">"subsample"</span>, np.arange(<span class="fl">0.6</span>, <span class="fl">1.</span>, <span class="fl">0.1</span>))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>Run 0 with subsample=0.6000 finished with score: 0.19204
Run 1 with subsample=0.7000 finished with score: 0.19204
Run 2 with subsample=0.8000 finished with score: 0.19200
Run 3 with subsample=0.9000 finished with score: 0.19207
Best run: subsample=0.7999999999999999 with score 0.191996017796013</code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-12-output-2.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T11:11:44.649608Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T11:11:44.649233Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T11:11:44.654989Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T11:11:44.653940Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T11:11:44.649574Z&quot;}" data-trusted="true" data-execution_count="18">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a>xgb_params[<span class="st">"subsample"</span>] <span class="op">=</span> <span class="fl">0.8</span></span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>We have now determined our final values for the <code>learning_rate</code>, <code>max_depth</code>, <code>colsample_bytree</code>, and <code>subsample</code> hyperparameters.</p>
<p>Let’s compute the test score:</p>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T11:12:21.516501Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T11:12:21.515560Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T11:12:48.480244Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T11:12:48.479200Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T11:12:21.516446Z&quot;}" data-trusted="true" data-execution_count="21">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb17"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb17-1"><a href="#cb17-1" aria-hidden="true" tabindex="-1"></a>model <span class="op">=</span> xgb.train(xgb_params,</span>
<span id="cb17-2"><a href="#cb17-2" aria-hidden="true" tabindex="-1"></a>                  dtrain<span class="op">=</span>xgb.DMatrix(data<span class="op">=</span>train_df[features], label<span class="op">=</span>train_df[target]),</span>
<span id="cb17-3"><a href="#cb17-3" aria-hidden="true" tabindex="-1"></a>                  num_boost_round<span class="op">=</span><span class="dv">1000</span>)</span>
<span id="cb17-4"><a href="#cb17-4" aria-hidden="true" tabindex="-1"></a>preds <span class="op">=</span> model.predict(xgb.DMatrix(data<span class="op">=</span>test_df[features]))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution="{&quot;iopub.execute_input&quot;:&quot;2023-03-30T11:12:48.732180Z&quot;,&quot;iopub.status.busy&quot;:&quot;2023-03-30T11:12:48.731782Z&quot;,&quot;iopub.status.idle&quot;:&quot;2023-03-30T11:12:48.819926Z&quot;,&quot;shell.execute_reply&quot;:&quot;2023-03-30T11:12:48.818786Z&quot;,&quot;shell.execute_reply.started&quot;:&quot;2023-03-30T11:12:48.732127Z&quot;}" data-trusted="true" data-execution_count="23">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb18"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb18-1"><a href="#cb18-1" aria-hidden="true" tabindex="-1"></a>log_loss(test_df[target].to_numpy(), preds)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="23">
<pre><code>0.19447658866057935</code></pre>
</div>
</div>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>