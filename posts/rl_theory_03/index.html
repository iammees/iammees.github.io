<!DOCTYPE html>
<html xmlns="http://www.w3.org/1999/xhtml" lang="en" xml:lang="en"><head>

<meta charset="utf-8">
<meta name="generator" content="quarto-1.3.290">

<meta name="viewport" content="width=device-width, initial-scale=1.0, user-scalable=yes">

<meta name="dcterms.date" content="2021-04-29">

<title>iammees - Theory of RL III: Dynamic Programming</title>
<style>
code{white-space: pre-wrap;}
span.smallcaps{font-variant: small-caps;}
div.columns{display: flex; gap: min(4vw, 1.5em);}
div.column{flex: auto; overflow-x: auto;}
div.hanging-indent{margin-left: 1.5em; text-indent: -1.5em;}
ul.task-list{list-style: none;}
ul.task-list li input[type="checkbox"] {
  width: 0.8em;
  margin: 0 0.8em 0.2em -1em; /* quarto-specific, see https://github.com/quarto-dev/quarto-cli/issues/4556 */ 
  vertical-align: middle;
}
/* CSS for syntax highlighting */
pre > code.sourceCode { white-space: pre; position: relative; }
pre > code.sourceCode > span { display: inline-block; line-height: 1.25; }
pre > code.sourceCode > span:empty { height: 1.2em; }
.sourceCode { overflow: visible; }
code.sourceCode > span { color: inherit; text-decoration: inherit; }
div.sourceCode { margin: 1em 0; }
pre.sourceCode { margin: 0; }
@media screen {
div.sourceCode { overflow: auto; }
}
@media print {
pre > code.sourceCode { white-space: pre-wrap; }
pre > code.sourceCode > span { text-indent: -5em; padding-left: 5em; }
}
pre.numberSource code
  { counter-reset: source-line 0; }
pre.numberSource code > span
  { position: relative; left: -4em; counter-increment: source-line; }
pre.numberSource code > span > a:first-child::before
  { content: counter(source-line);
    position: relative; left: -1em; text-align: right; vertical-align: baseline;
    border: none; display: inline-block;
    -webkit-touch-callout: none; -webkit-user-select: none;
    -khtml-user-select: none; -moz-user-select: none;
    -ms-user-select: none; user-select: none;
    padding: 0 4px; width: 4em;
  }
pre.numberSource { margin-left: 3em;  padding-left: 4px; }
div.sourceCode
  {   }
@media screen {
pre > code.sourceCode > span > a:first-child::before { text-decoration: underline; }
}
</style>


<script src="../../site_libs/quarto-nav/quarto-nav.js"></script>
<script src="../../site_libs/quarto-nav/headroom.min.js"></script>
<script src="../../site_libs/clipboard/clipboard.min.js"></script>
<script src="../../site_libs/quarto-search/autocomplete.umd.js"></script>
<script src="../../site_libs/quarto-search/fuse.min.js"></script>
<script src="../../site_libs/quarto-search/quarto-search.js"></script>
<meta name="quarto:offset" content="../../">
<script src="../../site_libs/quarto-html/quarto.js"></script>
<script src="../../site_libs/quarto-html/popper.min.js"></script>
<script src="../../site_libs/quarto-html/tippy.umd.min.js"></script>
<script src="../../site_libs/quarto-html/anchor.min.js"></script>
<link href="../../site_libs/quarto-html/tippy.css" rel="stylesheet">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-text-highlighting-styles">
<link href="../../site_libs/quarto-html/quarto-syntax-highlighting-dark.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-text-highlighting-styles">
<script src="../../site_libs/bootstrap/bootstrap.min.js"></script>
<link href="../../site_libs/bootstrap/bootstrap-icons.css" rel="stylesheet">
<link href="../../site_libs/bootstrap/bootstrap.min.css" rel="stylesheet" class="quarto-color-scheme" id="quarto-bootstrap" data-mode="light">
<link href="../../site_libs/bootstrap/bootstrap-dark.min.css" rel="prefetch" class="quarto-color-scheme quarto-color-alternate" id="quarto-bootstrap" data-mode="dark">
<script id="quarto-search-options" type="application/json">{
  "location": "navbar",
  "copy-button": false,
  "collapse-after": 3,
  "panel-placement": "end",
  "type": "overlay",
  "limit": 20,
  "language": {
    "search-no-results-text": "No results",
    "search-matching-documents-text": "matching documents",
    "search-copy-link-title": "Copy link to search",
    "search-hide-matches-text": "Hide additional matches",
    "search-more-match-text": "more match in this document",
    "search-more-matches-text": "more matches in this document",
    "search-clear-button-title": "Clear",
    "search-detached-cancel-button-title": "Cancel",
    "search-submit-button-title": "Submit"
  }
}</script>

  <script src="https://polyfill.io/v3/polyfill.min.js?features=es6"></script>
  <script src="https://cdn.jsdelivr.net/npm/mathjax@3/es5/tex-chtml-full.js" type="text/javascript"></script>

<link rel="stylesheet" href="../../styles.css">
</head>

<body class="nav-fixed fullcontent">

<div id="quarto-search-results"></div>
  <header id="quarto-header" class="headroom fixed-top">
    <nav class="navbar navbar-expand-lg navbar-dark ">
      <div class="navbar-container container-fluid">
      <div class="navbar-brand-container">
    <a class="navbar-brand" href="../../index.html">
    <span class="navbar-title">iammees</span>
    </a>
  </div>
            <div id="quarto-search" class="" title="Search"></div>
          <button class="navbar-toggler" type="button" data-bs-toggle="collapse" data-bs-target="#navbarCollapse" aria-controls="navbarCollapse" aria-expanded="false" aria-label="Toggle navigation" onclick="if (window.quartoToggleHeadroom) { window.quartoToggleHeadroom(); }">
  <span class="navbar-toggler-icon"></span>
</button>
          <div class="collapse navbar-collapse" id="navbarCollapse">
            <ul class="navbar-nav navbar-nav-scroll ms-auto">
  <li class="nav-item compact">
    <a class="nav-link" href="https://github.com/iammees" rel="" target=""><i class="bi bi-github" role="img">
</i> 
 <span class="menu-text"></span></a>
  </li>  
</ul>
            <div class="quarto-navbar-tools">
  <a href="" class="quarto-color-scheme-toggle quarto-navigation-tool  px-1" onclick="window.quartoToggleColorScheme(); return false;" title="Toggle dark mode"><i class="bi"></i></a>
</div>
          </div> <!-- /navcollapse -->
      </div> <!-- /container-fluid -->
    </nav>
</header>
<!-- content -->
<header id="title-block-header" class="quarto-title-block default page-columns page-full">
  <div class="quarto-title-banner page-columns page-full">
    <div class="quarto-title column-body">
      <h1 class="title">Theory of RL III: Dynamic Programming</h1>
                      </div>
  </div>
    
  
  <div class="quarto-title-meta">

      
      <div>
      <div class="quarto-title-meta-heading">Published</div>
      <div class="quarto-title-meta-contents">
        <p class="date">April 29, 2021</p>
      </div>
    </div>
    
      
    </div>
    
  
  </header><div id="quarto-content" class="quarto-container page-columns page-rows-contents page-layout-article page-navbar">
<!-- sidebar -->
<!-- margin-sidebar -->
    
<!-- main -->
<main class="content quarto-banner-title-block" id="quarto-document-content">




<p>Dynamic programming (DP) provides algorithms for <strong>solving the RL problem given that a perfect model of the environment is available</strong>. That is, assuming full access to the MDP, DP <strong>allows us to obtain optimal policies by finding the optimal value functions</strong>, <span class="math inline">\(v_*\)</span> or <span class="math inline">\(q_*\)</span>, that satisfy the Bellman optimality equations. As we have noted before, having full access to the MDP is not realistic when we face more complex problems. Nonetheless, DP provides the <strong>theoretical underpinning for many RL algorithms</strong> that try to approximate optimal solutions without perfect knowledge of an environment’s dynamics and is therefore essential for working on RL problems.</p>
<p>Note that we assume that we deal with a finite MDP. In particular, we assume that the state set <span class="math inline">\(\mathcal{S}\)</span>, action set <span class="math inline">\(\mathcal{A}\)</span>, and reward set <span class="math inline">\(\mathcal{R}\)</span> of the environment are all finite and that its dynamics are given by a set of probabilities <span class="math inline">\(p(s', r \ \vert \ s, a)\)</span>, for all <span class="math inline">\(s \in \mathcal{S}\)</span>, <span class="math inline">\(a \in \mathcal{A}(s)\)</span>, <span class="math inline">\(r \in \mathcal{R}\)</span>, and <span class="math inline">\(s' \in \mathcal{S^+}\)</span>. Still, it is possible to get approximate solutions for continuous problems with the same methods by quantizing the state and action spaces.</p>
<p>To apply the DP algorithms, we’ll use two custom <a href="https://github.com/openai/gym">gym</a> environments: <code>WalkEnv</code> and <code>LakeEnv</code>.</p>
<p>In <code>WalkEnv</code> the agent needs to reach the end (the rightmost state) of a corridor without falling into a hole (the leftmost state). Unfortunately, the corridor is slippery: the agent (starting in the middle) only moves in the intended direction with probability <span class="math inline">\(1/2\)</span>. With probability <span class="math inline">\(1/3\)</span> the agent stays in the same state, and with probability <span class="math inline">\(1/6\)</span> the agent trips and falls in the opposite direction.</p>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb1"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb1-1"><a href="#cb1-1" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> numpy <span class="im">as</span> np</span>
<span id="cb1-2"><a href="#cb1-2" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym</span>
<span id="cb1-3"><a href="#cb1-3" aria-hidden="true" tabindex="-1"></a><span class="im">import</span> gym_walk</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb2"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb2-1"><a href="#cb2-1" aria-hidden="true" tabindex="-1"></a>rng <span class="op">=</span> np.random.default_rng(<span class="dv">1</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="62">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb3"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb3-1"><a href="#cb3-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"SlipperyWalkFive-v0"</span>)</span>
<span id="cb3-2"><a href="#cb3-2" aria-hidden="true" tabindex="-1"></a>env.render(show_step<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-4-output-1.png" class="quarto-discovered-preview-image img-fluid"></p>
</div>
</div>
<p>In <code>LakeEnv</code> the problem is essentially the same, but slightly more complex: the agent has to cross a frozen lake without falling into a hole. Again, the ice is slippery so that the agent only moves in the intended direction with probability <span class="math inline">\(1/3\)</span>. With probability <span class="math inline">\(1/3\)</span> each agent slides to the left or right of the current position (in relation to the intended direction). Let’s see how this looks like:</p>
<div class="cell" data-execution_count="39">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb4"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb4-1"><a href="#cb4-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"FrozenLake4x4-v2"</span>)</span>
<span id="cb4-2"><a href="#cb4-2" aria-hidden="true" tabindex="-1"></a>env.render()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-5-output-1.png" class="img-fluid"></p>
</div>
</div>
<section id="policy-evaluation" class="level2">
<h2 class="anchored" data-anchor-id="policy-evaluation">Policy evaluation</h2>
<p>Our first DP algorithm is <strong>(iterative) policy evaluation</strong>. The algorithm computes the state-value function <span class="math inline">\(v_{\pi}\)</span> for a given policy <span class="math inline">\(\pi\)</span>, thereby solving the so-called <strong>prediction problem</strong>. Essentially, the algorithm uses the <strong>Bellman equation as an update rule</strong> for iteratively improving the approximation of the value function:</p>
<p><span class="math display">\[v_{k+1}(s)= \mathbb{E}_{\pi} \left[ R_{t+1} + \gamma v_k (S_{t+1})\ \vert\ S_t = s \right] =\sum_a \pi(a \vert s) \sum_{s', r} p(s', r \vert s, a) \big[r + \gamma v_k(s')\big]\]</span></p>
<p>Let’s go over what happens here. We start by initializing <span class="math inline">\(v_0(s)\)</span> for all <span class="math inline">\(s \in \mathcal{S}\)</span> arbitrarily, and to <span class="math inline">\(0\)</span> if <span class="math inline">\(s\)</span> is a terminal state. Then, at each iteration <span class="math inline">\(k\)</span>, we compute a new approximation of each state-value by adding the expected immediate reward from that state and the estimated value of its successor state. This is called an <strong>expected update</strong> since it is based on an expectation over all possible transitions from that state. As <span class="math inline">\(k\)</span> approaches infinity, the algorithm converges to the true values (in practice we simply stop updating when the updates become negligible).</p>
<p>Importantly, at each iteration <span class="math inline">\(k\)</span> we use an estimate from the previous iteration, <span class="math inline">\(v_{k-1}(s')\)</span>, to compute the new estimate <span class="math inline">\(v_k(s)\)</span>. Using an estimate when calculating an estimate is a technique called <strong>bootstrapping</strong> that commonly appears in RL.</p>
<section id="implementation" class="level3">
<h3 class="anchored" data-anchor-id="implementation">Implementation</h3>
<p>Before we look at a possible implementation of the policy evaluation algorithm, we first need to introduce the transition probability matrix <code>prob</code> and the reward matrix <code>reward</code>.</p>
<div class="cell" data-execution_count="63">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb5"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb5-1"><a href="#cb5-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"SlipperyWalkFive-v0"</span>)</span>
<span id="cb5-2"><a href="#cb5-2" aria-hidden="true" tabindex="-1"></a>LEFT, RIGHT <span class="op">=</span> <span class="dv">0</span>, <span class="dv">1</span></span>
<span id="cb5-3"><a href="#cb5-3" aria-hidden="true" tabindex="-1"></a>env.render(show_step<span class="op">=</span><span class="va">False</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-6-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="41">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb6"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb6-1"><a href="#cb6-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> np.printoptions(precision<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb6-2"><a href="#cb6-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(env.prob)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[[1.    0.    0.    0.    0.    0.    0.   ]
  [1.    0.    0.    0.    0.    0.    0.   ]]

 [[0.5   0.333 0.167 0.    0.    0.    0.   ]
  [0.167 0.333 0.5   0.    0.    0.    0.   ]]

 [[0.    0.5   0.333 0.167 0.    0.    0.   ]
  [0.    0.167 0.333 0.5   0.    0.    0.   ]]

 [[0.    0.    0.5   0.333 0.167 0.    0.   ]
  [0.    0.    0.167 0.333 0.5   0.    0.   ]]

 [[0.    0.    0.    0.5   0.333 0.167 0.   ]
  [0.    0.    0.    0.167 0.333 0.5   0.   ]]

 [[0.    0.    0.    0.    0.5   0.333 0.167]
  [0.    0.    0.    0.    0.167 0.333 0.5  ]]

 [[0.    0.    0.    0.    0.    0.    1.   ]
  [0.    0.    0.    0.    0.    0.    1.   ]]]</code></pre>
</div>
</div>
<p>The transition probability matrix <code>prob</code> shows the transition probabilities depending on the current state and action. For example, suppose that the agent is currently in state <span class="math inline">\(5\)</span> (the second state from the right). Look at the second row: if it chooses the action “RIGHT”, it will reach the rightmost state with probability <span class="math inline">\(0.5\)</span>, stay in the same state with probability <span class="math inline">\(0.333\)</span>, and trip and fall backwards with probability <span class="math inline">\(0.167\)</span>. Accordingly, the first row corresponds to the action “LEFT”:</p>
<div class="cell" data-execution_count="42">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb8"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb8-1"><a href="#cb8-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> np.printoptions(precision<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb8-2"><a href="#cb8-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(env.prob[<span class="dv">5</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[0.    0.    0.    0.    0.5   0.333 0.167]
 [0.    0.    0.    0.    0.167 0.333 0.5  ]]</code></pre>
</div>
</div>
<p>Now, suppose the agent fell into the hole (that is, it is in state <span class="math inline">\(0\)</span>, the leftmost state). Since this is a terminal state, the agent cannot leave it. Regardless of the action, the transition is always to the same state with probability <span class="math inline">\(1.0\)</span>.</p>
<div class="cell" data-execution_count="43">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb10"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb10-1"><a href="#cb10-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> np.printoptions(precision<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb10-2"><a href="#cb10-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(env.prob[<span class="dv">0</span>])</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[1. 0. 0. 0. 0. 0. 0.]
 [1. 0. 0. 0. 0. 0. 0.]]</code></pre>
</div>
</div>
<p>The transition probability matrix has the shape <code>(n_states, n_actions, n_states)</code>. For every state it shows the transition probabilities to the next states depending on the action.</p>
<p>The reward matrix <code>reward</code> has the same shape. For every state it shows the potential rewards depending on the action. In our case, we use a very simple reward scheme. The agent gets reward <span class="math inline">\(1.0\)</span> if it reaches the rightmost state. As we see below, this is only possible from state <span class="math inline">\(5\)</span> where the agent can intentionally reach the goal by taking the action “RIGHT” or unintentionally by tripping and falling backward when choosing “LEFT”.</p>
<div class="cell" data-execution_count="44">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb12"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb12-1"><a href="#cb12-1" aria-hidden="true" tabindex="-1"></a><span class="cf">with</span> np.printoptions(precision<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb12-2"><a href="#cb12-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(env.reward)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>[[[0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0.]]

 [[0. 0. 0. 0. 0. 0. 1.]
  [0. 0. 0. 0. 0. 0. 1.]]

 [[0. 0. 0. 0. 0. 0. 0.]
  [0. 0. 0. 0. 0. 0. 0.]]]</code></pre>
</div>
</div>
<p>Together <code>prob</code> and <code>reward</code> represent complete knowledge of the environment. We can now turn our attention to a possible implementation of the policy evaluation algorithm:</p>
<div class="cell" data-execution_count="45">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb14"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb14-1"><a href="#cb14-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_evaluation(pi, prob, reward, gamma<span class="op">=</span><span class="fl">1.</span>, theta<span class="op">=</span><span class="fl">1e-10</span>):</span>
<span id="cb14-2"><a href="#cb14-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb14-3"><a href="#cb14-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb14-4"><a href="#cb14-4" aria-hidden="true" tabindex="-1"></a><span class="co">        pi (np.ndarray): policy;</span></span>
<span id="cb14-5"><a href="#cb14-5" aria-hidden="true" tabindex="-1"></a><span class="co">        prob (np.ndarray): transition probability matrix;</span></span>
<span id="cb14-6"><a href="#cb14-6" aria-hidden="true" tabindex="-1"></a><span class="co">        reward (np.ndarray): reward matrix;</span></span>
<span id="cb14-7"><a href="#cb14-7" aria-hidden="true" tabindex="-1"></a><span class="co">        gamma (float): discount factor;</span></span>
<span id="cb14-8"><a href="#cb14-8" aria-hidden="true" tabindex="-1"></a><span class="co">        theta (float): accuracy threshold;</span></span>
<span id="cb14-9"><a href="#cb14-9" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb14-10"><a href="#cb14-10" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb14-11"><a href="#cb14-11" aria-hidden="true" tabindex="-1"></a><span class="co">        value (np.ndarray): value function of the given policy;</span></span>
<span id="cb14-12"><a href="#cb14-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb14-13"><a href="#cb14-13" aria-hidden="true" tabindex="-1"></a>    n_states, n_actions <span class="op">=</span> prob.shape[<span class="dv">0</span>], prob.shape[<span class="dv">1</span>]</span>
<span id="cb14-14"><a href="#cb14-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the values to zero</span></span>
<span id="cb14-15"><a href="#cb14-15" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> np.zeros(n_states)</span>
<span id="cb14-16"><a href="#cb14-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb14-17"><a href="#cb14-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keep track of the old values</span></span>
<span id="cb14-18"><a href="#cb14-18" aria-hidden="true" tabindex="-1"></a>        value_old <span class="op">=</span> value.copy()</span>
<span id="cb14-19"><a href="#cb14-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop over all states</span></span>
<span id="cb14-20"><a href="#cb14-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(n_states):</span>
<span id="cb14-21"><a href="#cb14-21" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get the transition probabilities for the given state and action (as determined by the given policy)</span></span>
<span id="cb14-22"><a href="#cb14-22" aria-hidden="true" tabindex="-1"></a>            prob_sa <span class="op">=</span> prob[state, pi[state], :]</span>
<span id="cb14-23"><a href="#cb14-23" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get the possible rewards for the given state and action</span></span>
<span id="cb14-24"><a href="#cb14-24" aria-hidden="true" tabindex="-1"></a>            reward_sa <span class="op">=</span> reward[state, pi[state], :]</span>
<span id="cb14-25"><a href="#cb14-25" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Perform the update (we'll look at this step in detail below)</span></span>
<span id="cb14-26"><a href="#cb14-26" aria-hidden="true" tabindex="-1"></a>            value[state] <span class="op">=</span> prob_sa.T.dot(reward_sa <span class="op">+</span> gamma <span class="op">*</span> value_old)</span>
<span id="cb14-27"><a href="#cb14-27" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Break the loop if the update is below the threshold theta</span></span>
<span id="cb14-28"><a href="#cb14-28" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">all</span>(np.<span class="bu">abs</span>(value <span class="op">-</span> value_old) <span class="op">&lt;</span> theta):</span>
<span id="cb14-29"><a href="#cb14-29" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb14-30"><a href="#cb14-30" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> value</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>It is probably wise to look at the update <code>value[state] = prob_sa.T.dot(reward_sa + gamma * value_old)</code> in isolation. What happens here? Let’s make up a simplified example. Suppose we are in state <span class="math inline">\(0\)</span> and, following our policy, choose the action <span class="math inline">\(1\)</span>. First, we need to get the probabilities of all possible transitions by accessing <code>prob[0, 1, :]</code>. In our example that might simply be <code>[0.5, 0.5]</code>, that is, state <span class="math inline">\(0\)</span> and state <span class="math inline">\(1\)</span> are the only possible next states and equally likely. Next we need the possible rewards. <code>reward[0, 1, :]</code> gives <code>[2.0, 10.0]</code>, that is, we get the reward <span class="math inline">\(2\)</span> when we stay in the same state <span class="math inline">\(0\)</span> and reward <span class="math inline">\(10\)</span> when we transition to state <span class="math inline">\(1\)</span>. Finally, suppose that our current value estimates are <code>[1.0, 1.0]</code> and we chose <code>gamma = 1.0</code>. Our update to the value of state <span class="math inline">\(0\)</span> would look like this: <span class="math inline">\(0.5 * (2.0 + 1.0 * 1.0) + 0.5 * (10.0 + 1.0 * 1.0)\)</span>. To simplify this in the implementation above, we use the dot product (or inner product) operation.</p>
<p>We are now ready to apply the policy evaluation algorithm to our <code>WalkEnv</code> example. Let’s use it to compare the value functions of three policies: <code>pi_left</code> which chooses “LEFT” in every state, <code>pi_right</code> which chooses “RIGHT” every time (logically, this is the optimal policy in this example), and <code>pi_random</code> which randomly picks a direction in each step.</p>
<div class="cell" data-execution_count="46">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb15"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb15-1"><a href="#cb15-1" aria-hidden="true" tabindex="-1"></a>pi_left <span class="op">=</span> {state : LEFT <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(env.prob.shape[<span class="dv">0</span>])}</span>
<span id="cb15-2"><a href="#cb15-2" aria-hidden="true" tabindex="-1"></a>pi_right <span class="op">=</span> {state : RIGHT <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(env.prob.shape[<span class="dv">0</span>])}</span>
<span id="cb15-3"><a href="#cb15-3" aria-hidden="true" tabindex="-1"></a>pi_random <span class="op">=</span> {state : rng.choice(env.prob.shape[<span class="dv">1</span>]) <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(env.prob.shape[<span class="dv">0</span>])}</span>
<span id="cb15-4"><a href="#cb15-4" aria-hidden="true" tabindex="-1"></a>policies <span class="op">=</span> {<span class="st">"pi_left"</span>: pi_left, <span class="st">"pi_right"</span>: pi_right, <span class="st">"pi_random"</span>: pi_random}</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="47">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb16"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb16-1"><a href="#cb16-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pi_name, pi <span class="kw">in</span> policies.items():</span>
<span id="cb16-2"><a href="#cb16-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>pi_name<span class="sc">}</span><span class="ss">: "</span>)</span>
<span id="cb16-3"><a href="#cb16-3" aria-hidden="true" tabindex="-1"></a>    env.render(policy<span class="op">=</span>pi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>pi_left: </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>pi_right: </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>pi_random: </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-13-output-6.png" class="img-fluid"></p>
</div>
</div>
<p>We can now determine the corresponding value functions with the help of our <code>policy_evaluation</code> algorithm:</p>
<div class="cell" data-execution_count="48">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb20"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb20-1"><a href="#cb20-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pi_name, pi <span class="kw">in</span> policies.items():</span>
<span id="cb20-2"><a href="#cb20-2" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> policy_evaluation(pi, env.prob, env.reward)</span>
<span id="cb20-3"><a href="#cb20-3" aria-hidden="true" tabindex="-1"></a>    <span class="cf">with</span> np.printoptions(precision<span class="op">=</span><span class="dv">3</span>):</span>
<span id="cb20-4"><a href="#cb20-4" aria-hidden="true" tabindex="-1"></a>        <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>pi_name<span class="sc">}</span><span class="ss">:</span><span class="ch">\t</span><span class="ss"> </span><span class="sc">{</span>value<span class="sc">}</span><span class="ss">"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>pi_left:     [0.    0.003 0.011 0.036 0.11  0.332 0.   ]
pi_right:    [0.    0.668 0.89  0.964 0.989 0.997 0.   ]
pi_random:   [0.    0.519 0.692 0.75  0.769 0.827 0.   ]</code></pre>
</div>
</div>
<p>Again, it is probably easier to visualize the value functions.</p>
<div class="cell" data-execution_count="49">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb22"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb22-1"><a href="#cb22-1" aria-hidden="true" tabindex="-1"></a><span class="cf">for</span> pi_name, pi <span class="kw">in</span> policies.items():</span>
<span id="cb22-2"><a href="#cb22-2" aria-hidden="true" tabindex="-1"></a>    <span class="bu">print</span>(<span class="ss">f"</span><span class="sc">{</span>pi_name<span class="sc">}</span><span class="ss">: "</span>)</span>
<span id="cb22-3"><a href="#cb22-3" aria-hidden="true" tabindex="-1"></a>    env.render(policy<span class="op">=</span>pi, values<span class="op">=</span>policy_evaluation(pi, env.prob, env.reward))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-stdout">
<pre><code>pi_left: </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-2.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>pi_right: </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-4.png" class="img-fluid"></p>
</div>
<div class="cell-output cell-output-stdout">
<pre><code>pi_random: </code></pre>
</div>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-15-output-6.png" class="img-fluid"></p>
</div>
</div>
</section>
</section>
<section id="policy-improvement" class="level2">
<h2 class="anchored" data-anchor-id="policy-improvement">Policy Improvement</h2>
<p>With policy evaluation we now have an algorithm to evaluate policies. How can we use it to obtain better policies, a task that is typically referred to as the <strong>control problem</strong>? Imagine we have computed the value function for an arbitrary (deterministic) policy <span class="math inline">\(\pi\)</span> and look at a specific state <span class="math inline">\(s\)</span>. Since we know its value, we know how valuable it is to follow policy <span class="math inline">\(\pi\)</span> starting from this state. But is there actually an action <span class="math inline">\(a \neq \pi(s)\)</span> we could select now which puts us in a better position (assuming we still follow policy <span class="math inline">\(\pi\)</span> afterwards)? To answer this question, we can use the state-value function <span class="math inline">\(v_{\pi}\)</span> and the known dynamics of the MDP to <strong>calculate the action-value function <span class="math inline">\(q_{\pi}\)</span></strong>. The Q-function will tell us how valuable every state-action is, and if, for a given state, there is an action that is more valuable than the action determined by policy <span class="math inline">\(\pi\)</span>. Then improving our policy is straightforward: in every state we just act greedily with respect to the action-value function (i.e.&nbsp;choose the actions with the highest values) and thereby obtain a new, improved policy <span class="math inline">\(\pi'\)</span>.</p>
<p>This algorithm is called <strong>policy improvement</strong> and, as the name suggests, guarantees to always find an improved policy <span class="math inline">\(\pi'\)</span> (unless, of course, the policy <span class="math inline">\(\pi\)</span> is already optimal). Let’s look at the math.</p>
<p>First, we need to compute <span class="math inline">\(q_{\pi}(s,a)\)</span>: <span class="math display">\[
\begin{align}
q_{\pi}(s,a) &amp; = \mathbb{E} \left[ R_{t+1} + \gamma v_{\pi} (S_{t+1})\ \vert\ S_t = s, A_t = a \right]\\
             &amp; = \sum_{s', r} p(s', r \vert s, a) \left[ r + \gamma v_{\pi}(s') \right]
\end{align}
\]</span></p>
<p>Then, we obtain the improved policy <span class="math inline">\(\pi'\)</span> by taking the highest-valued action in every state. Ties can be broken arbitrarily or, in the stochastic case, by assigning each action a specific probability (while assigning zero probability to all actions that are not maximal):</p>
<p><span class="math display">\[\pi'(s) = \text{argmax}_a\ q_{\pi}(s,a)\]</span></p>
<section id="implementation-1" class="level3">
<h3 class="anchored" data-anchor-id="implementation-1">Implementation</h3>
<div class="cell" data-execution_count="50">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb26"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb26-1"><a href="#cb26-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_improvement(value, prob, reward, gamma<span class="op">=</span><span class="fl">1.</span>):</span>
<span id="cb26-2"><a href="#cb26-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb26-3"><a href="#cb26-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb26-4"><a href="#cb26-4" aria-hidden="true" tabindex="-1"></a><span class="co">        value (np.ndarray): value function;</span></span>
<span id="cb26-5"><a href="#cb26-5" aria-hidden="true" tabindex="-1"></a><span class="co">        prob (np.ndarray): transition probability matrix;</span></span>
<span id="cb26-6"><a href="#cb26-6" aria-hidden="true" tabindex="-1"></a><span class="co">        reward (np.ndarray): reward matrix;</span></span>
<span id="cb26-7"><a href="#cb26-7" aria-hidden="true" tabindex="-1"></a><span class="co">        gamma (float): discount factor;</span></span>
<span id="cb26-8"><a href="#cb26-8" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb26-9"><a href="#cb26-9" aria-hidden="true" tabindex="-1"></a><span class="co">        pi (np.ndarray): policy;</span></span>
<span id="cb26-10"><a href="#cb26-10" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb26-11"><a href="#cb26-11" aria-hidden="true" tabindex="-1"></a>    n_states, n_actions <span class="op">=</span> prob.shape[<span class="dv">0</span>], prob.shape[<span class="dv">1</span>]</span>
<span id="cb26-12"><a href="#cb26-12" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the action-values to zero</span></span>
<span id="cb26-13"><a href="#cb26-13" aria-hidden="true" tabindex="-1"></a>    q <span class="op">=</span> np.zeros((n_states, n_actions))</span>
<span id="cb26-14"><a href="#cb26-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Loop over all states</span></span>
<span id="cb26-15"><a href="#cb26-15" aria-hidden="true" tabindex="-1"></a>    <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(n_states):</span>
<span id="cb26-16"><a href="#cb26-16" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop over all actions</span></span>
<span id="cb26-17"><a href="#cb26-17" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> action <span class="kw">in</span> <span class="bu">range</span>(n_actions):</span>
<span id="cb26-18"><a href="#cb26-18" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get the transition probabilities for the given state and action</span></span>
<span id="cb26-19"><a href="#cb26-19" aria-hidden="true" tabindex="-1"></a>            prob_sa <span class="op">=</span> prob[state, action, :]</span>
<span id="cb26-20"><a href="#cb26-20" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Get the rewards for the given state and action</span></span>
<span id="cb26-21"><a href="#cb26-21" aria-hidden="true" tabindex="-1"></a>            reward_sa <span class="op">=</span> reward[state, action, :]</span>
<span id="cb26-22"><a href="#cb26-22" aria-hidden="true" tabindex="-1"></a>            <span class="co"># Determine the action-value</span></span>
<span id="cb26-23"><a href="#cb26-23" aria-hidden="true" tabindex="-1"></a>            q[state][action] <span class="op">=</span> prob_sa.T.dot(reward_sa <span class="op">+</span> gamma <span class="op">*</span> value)</span>
<span id="cb26-24"><a href="#cb26-24" aria-hidden="true" tabindex="-1"></a>    <span class="co"># The improved policy uses the best available action in each state</span></span>
<span id="cb26-25"><a href="#cb26-25" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> np.argmax(q, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb26-26"><a href="#cb26-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> pi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s say we want to improve the policy <code>pi_left</code> that we introduced above. We begin by computing the corresponding value function:</p>
<div class="cell" data-execution_count="51">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb27"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb27-1"><a href="#cb27-1" aria-hidden="true" tabindex="-1"></a>value_pi_left <span class="op">=</span> policy_evaluation(pi_left, env.prob, env.reward)</span>
<span id="cb27-2"><a href="#cb27-2" aria-hidden="true" tabindex="-1"></a>env.render(policy<span class="op">=</span>pi_left, values<span class="op">=</span>value_pi_left)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-17-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Then we can apply the <code>policy_improvement</code> algorithm:</p>
<div class="cell" data-execution_count="52">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb28"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb28-1"><a href="#cb28-1" aria-hidden="true" tabindex="-1"></a>improved_pi <span class="op">=</span> policy_improvement(value_pi_left, env.prob, env.reward)</span>
<span id="cb28-2"><a href="#cb28-2" aria-hidden="true" tabindex="-1"></a>env.render(policy<span class="op">=</span>improved_pi, values<span class="op">=</span>policy_evaluation(improved_pi, env.prob, env.reward))</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-18-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>In our small example, a single iteration of the policy improvement algorithm actually sufficed to obtain an optimal policy of always moving right (in this case, <em>the</em> optimal policy). But this is not guaranteed! Let’s look at a more complex example:</p>
<div class="cell" data-execution_count="53">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb29"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb29-1"><a href="#cb29-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"FrozenLake4x4-v2"</span>)</span>
<span id="cb29-2"><a href="#cb29-2" aria-hidden="true" tabindex="-1"></a>env.render()</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-19-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>To say the least, the <code>pi_left</code> policy isn’t a particularly wise choice for this problem. Let’s see what happens when we apply our policy improvement algorithm:</p>
<div class="cell" data-execution_count="54">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb30"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb30-1"><a href="#cb30-1" aria-hidden="true" tabindex="-1"></a>pi_left <span class="op">=</span> {state : LEFT <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(env.prob.shape[<span class="dv">0</span>])}</span>
<span id="cb30-2"><a href="#cb30-2" aria-hidden="true" tabindex="-1"></a>value_pi_left <span class="op">=</span> policy_evaluation(pi_left, env.prob, env.reward)</span>
<span id="cb30-3"><a href="#cb30-3" aria-hidden="true" tabindex="-1"></a>env.render(policy<span class="op">=</span>pi_left, values<span class="op">=</span>value_pi_left)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-20-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="55">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb31"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb31-1"><a href="#cb31-1" aria-hidden="true" tabindex="-1"></a>improved_pi <span class="op">=</span> policy_improvement(value_pi_left, env.prob, env.reward)</span>
<span id="cb31-2"><a href="#cb31-2" aria-hidden="true" tabindex="-1"></a>value_improved_pi <span class="op">=</span> policy_evaluation(improved_pi, env.prob, env.reward)</span>
<span id="cb31-3"><a href="#cb31-3" aria-hidden="true" tabindex="-1"></a>env.render(policy<span class="op">=</span>improved_pi, values<span class="op">=</span>value_improved_pi)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-21-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>The new policy obtained by running policy improvement once is indeed better than the <code>pi_left</code> policy, but it is still far from optimal. What should we do now?</p>
</section>
</section>
<section id="policy-iteration" class="level2">
<h2 class="anchored" data-anchor-id="policy-iteration">Policy Iteration</h2>
<p>We now have an algorithm that computes the value function for a given policy (policy evaluation) and an algorithm that uses the value function to obtain a strictly better policy (policy iteration). Since there is only a finite number of policies in a finite MDP, we can <strong>obtain an optimal policy by alternating between policy evaluation and policy improvement</strong> until the policy doesn’t improve anymore. This process is called <strong>policy iteration</strong> and is <strong>guaranteed to converge</strong> to an optimal policy and the optimal value function regardless of the initial policy we choose.</p>
<p><span class="math display">\[\pi_0 \xrightarrow[]{\text{evaluate}} v_{\pi_0} \xrightarrow[]{\text{improve}}
\pi_1 \xrightarrow[]{\text{evaluate}} v_{\pi_1} \xrightarrow[]{\text{improve}}
\pi_2 \xrightarrow[]{\text{evaluate}}\ \dots\ \xrightarrow[]{\text{improve}}
\pi_* \xrightarrow[]{\text{evaluate}} v_*\]</span></p>
<section id="implementation-2" class="level3">
<h3 class="anchored" data-anchor-id="implementation-2">Implementation</h3>
<p>Since we already have policy evaluation and policy improvement at our disposal, implementing the policy iteration algorithm is pretty straightforward:</p>
<div class="cell" data-execution_count="56">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb32"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb32-1"><a href="#cb32-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> policy_iteration(prob, reward, gamma<span class="op">=</span><span class="fl">1.</span>, theta<span class="op">=</span><span class="fl">1e-10</span>):</span>
<span id="cb32-2"><a href="#cb32-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb32-3"><a href="#cb32-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb32-4"><a href="#cb32-4" aria-hidden="true" tabindex="-1"></a><span class="co">        prob (np.ndarray): transition probability matrix;</span></span>
<span id="cb32-5"><a href="#cb32-5" aria-hidden="true" tabindex="-1"></a><span class="co">        reward (np.ndarray): reward matrix;</span></span>
<span id="cb32-6"><a href="#cb32-6" aria-hidden="true" tabindex="-1"></a><span class="co">        gamma (float): discount factor;</span></span>
<span id="cb32-7"><a href="#cb32-7" aria-hidden="true" tabindex="-1"></a><span class="co">        theta (float): accuracy threshold;</span></span>
<span id="cb32-8"><a href="#cb32-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb32-9"><a href="#cb32-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb32-10"><a href="#cb32-10" aria-hidden="true" tabindex="-1"></a><span class="co">        value (np.ndarray): optimal value function;</span></span>
<span id="cb32-11"><a href="#cb32-11" aria-hidden="true" tabindex="-1"></a><span class="co">        pi (np.ndarray): optimal policy;</span></span>
<span id="cb32-12"><a href="#cb32-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb32-13"><a href="#cb32-13" aria-hidden="true" tabindex="-1"></a>    n_states, n_actions <span class="op">=</span> prob.shape[<span class="dv">0</span>], prob.shape[<span class="dv">1</span>]</span>
<span id="cb32-14"><a href="#cb32-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Randomly initialize a policy</span></span>
<span id="cb32-15"><a href="#cb32-15" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> np.random.choice(n_actions, n_states)</span>
<span id="cb32-16"><a href="#cb32-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb32-17"><a href="#cb32-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Keep a copy of the old policy</span></span>
<span id="cb32-18"><a href="#cb32-18" aria-hidden="true" tabindex="-1"></a>        old_pi <span class="op">=</span> pi.copy()</span>
<span id="cb32-19"><a href="#cb32-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Compute the value function of the policy</span></span>
<span id="cb32-20"><a href="#cb32-20" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> policy_evaluation(pi, prob, reward, gamma, theta)</span>
<span id="cb32-21"><a href="#cb32-21" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Obtain an improved policy by running policy improvement</span></span>
<span id="cb32-22"><a href="#cb32-22" aria-hidden="true" tabindex="-1"></a>        pi <span class="op">=</span> policy_improvement(value, prob, reward, gamma)</span>
<span id="cb32-23"><a href="#cb32-23" aria-hidden="true" tabindex="-1"></a>        <span class="co"># If the policy does not change anymore, we have obtained an optimal policy</span></span>
<span id="cb32-24"><a href="#cb32-24" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.array_equal(pi, old_pi):</span>
<span id="cb32-25"><a href="#cb32-25" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb32-26"><a href="#cb32-26" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> value, pi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s use <code>policy_iteration</code> to obtain an optimal policy for our <code>LakeEnv</code>:</p>
<div class="cell" data-execution_count="57">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb33"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb33-1"><a href="#cb33-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"FrozenLake4x4-v2"</span>)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<div class="cell" data-execution_count="58">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb34"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb34-1"><a href="#cb34-1" aria-hidden="true" tabindex="-1"></a>value_star, pi_star <span class="op">=</span> policy_iteration(env.prob, env.reward)</span>
<span id="cb34-2"><a href="#cb34-2" aria-hidden="true" tabindex="-1"></a>env.render(values<span class="op">=</span>value_star, policy<span class="op">=</span>pi_star)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-24-output-1.png" class="img-fluid"></p>
</div>
</div>
<p>Interesting, isn’t it?</p>
</section>
</section>
<section id="value-iteration" class="level2">
<h2 class="anchored" data-anchor-id="value-iteration">Value Iteration</h2>
<p>One drawback of the policy iteration algorithm is that each of its iterations involves policy evaluation which usually requires an iterative computation itself. Fortunately, it turns out that exact convergence of the policy evaluation algorithm in every step of policy iteration is <em>not</em> necessary. Indeed, we can truncate policy evaluation without losing the convergence guarantees of policy iteration. If we <strong>stop policy evaluation after a <em>single</em> sweep of the state space</strong> (that is, after one update of each state), we obtain an algorithm called <strong>value iteration</strong> that can be expressed in a single equation effectively combining one sweep of policy evaluation and one sweep of policy improvement:</p>
<p><span class="math display">\[
\begin{align}
v_{k+1}(s) &amp; = \max_a \mathbb{E} \left[ R_{t+1} + \gamma v_k (S_{t+1})\ \vert\ S_t = s, A_t = a \right]\\
           &amp; = \max_a \sum_{s', r} p(s', r \vert s, a) \left[ r + \gamma v_k(s') \right]
\end{align}
\]</span></p>
<p>For each state, we calculate the value of each action over all possible transitions as the sum of the immediate reward and the discounted estimate of the next state’s value weighted by the probability of the transition. Then, we simply take the <span class="math inline">\(\max\)</span> over all actions. You may recognize that this is the same as <strong>turning the Bellman optimality equation into an update rule</strong> and identical to the policy evaluation update except for the <span class="math inline">\(\max\)</span> over all actions.</p>
<section id="implementation-3" class="level3">
<h3 class="anchored" data-anchor-id="implementation-3">Implementation</h3>
<div class="cell" data-execution_count="59">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb35"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb35-1"><a href="#cb35-1" aria-hidden="true" tabindex="-1"></a><span class="kw">def</span> value_iteration(prob, reward, gamma<span class="op">=</span><span class="fl">1.</span>, theta<span class="op">=</span><span class="fl">1e-10</span>):</span>
<span id="cb35-2"><a href="#cb35-2" aria-hidden="true" tabindex="-1"></a>    <span class="co">"""</span></span>
<span id="cb35-3"><a href="#cb35-3" aria-hidden="true" tabindex="-1"></a><span class="co">    Args:</span></span>
<span id="cb35-4"><a href="#cb35-4" aria-hidden="true" tabindex="-1"></a><span class="co">        prob (np.ndarray): transition probability matrix;</span></span>
<span id="cb35-5"><a href="#cb35-5" aria-hidden="true" tabindex="-1"></a><span class="co">        reward (np.ndarray): reward matrix;</span></span>
<span id="cb35-6"><a href="#cb35-6" aria-hidden="true" tabindex="-1"></a><span class="co">        gamma (float): discount factor;</span></span>
<span id="cb35-7"><a href="#cb35-7" aria-hidden="true" tabindex="-1"></a><span class="co">        theta (float): accuracy threshold;</span></span>
<span id="cb35-8"><a href="#cb35-8" aria-hidden="true" tabindex="-1"></a><span class="co">    </span></span>
<span id="cb35-9"><a href="#cb35-9" aria-hidden="true" tabindex="-1"></a><span class="co">    Returns:</span></span>
<span id="cb35-10"><a href="#cb35-10" aria-hidden="true" tabindex="-1"></a><span class="co">        value (np.ndarray): optimal value function;</span></span>
<span id="cb35-11"><a href="#cb35-11" aria-hidden="true" tabindex="-1"></a><span class="co">        pi (np.ndarray): optimal policy;</span></span>
<span id="cb35-12"><a href="#cb35-12" aria-hidden="true" tabindex="-1"></a><span class="co">    """</span></span>
<span id="cb35-13"><a href="#cb35-13" aria-hidden="true" tabindex="-1"></a>    n_states, n_actions <span class="op">=</span> prob.shape[<span class="dv">0</span>], prob.shape[<span class="dv">1</span>]</span>
<span id="cb35-14"><a href="#cb35-14" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Initialize the values to 0</span></span>
<span id="cb35-15"><a href="#cb35-15" aria-hidden="true" tabindex="-1"></a>    value <span class="op">=</span> np.zeros(n_states)</span>
<span id="cb35-16"><a href="#cb35-16" aria-hidden="true" tabindex="-1"></a>    <span class="cf">while</span> <span class="va">True</span>:</span>
<span id="cb35-17"><a href="#cb35-17" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Initialize the action-values to 0</span></span>
<span id="cb35-18"><a href="#cb35-18" aria-hidden="true" tabindex="-1"></a>        q <span class="op">=</span> np.zeros((n_states, n_actions))</span>
<span id="cb35-19"><a href="#cb35-19" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Loop over all states and actions</span></span>
<span id="cb35-20"><a href="#cb35-20" aria-hidden="true" tabindex="-1"></a>        <span class="cf">for</span> state <span class="kw">in</span> <span class="bu">range</span>(n_states):</span>
<span id="cb35-21"><a href="#cb35-21" aria-hidden="true" tabindex="-1"></a>            <span class="cf">for</span> action <span class="kw">in</span> <span class="bu">range</span>(n_actions):</span>
<span id="cb35-22"><a href="#cb35-22" aria-hidden="true" tabindex="-1"></a>                <span class="co"># This update step should be familiar by now</span></span>
<span id="cb35-23"><a href="#cb35-23" aria-hidden="true" tabindex="-1"></a>                prob_sa <span class="op">=</span> prob[state, action, :]</span>
<span id="cb35-24"><a href="#cb35-24" aria-hidden="true" tabindex="-1"></a>                reward_sa <span class="op">=</span> reward[state, action, :]</span>
<span id="cb35-25"><a href="#cb35-25" aria-hidden="true" tabindex="-1"></a>                q[state][action] <span class="op">=</span> prob_sa.T.dot(reward_sa <span class="op">+</span> gamma <span class="op">*</span> value)</span>
<span id="cb35-26"><a href="#cb35-26" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Stop iterating ff the changes to the value function become negligible</span></span>
<span id="cb35-27"><a href="#cb35-27" aria-hidden="true" tabindex="-1"></a>        <span class="cf">if</span> np.<span class="bu">all</span>(np.<span class="bu">abs</span>(value <span class="op">-</span> np.<span class="bu">max</span>(q, axis<span class="op">=</span><span class="dv">1</span>)) <span class="op">&lt;</span> theta):</span>
<span id="cb35-28"><a href="#cb35-28" aria-hidden="true" tabindex="-1"></a>            <span class="cf">break</span></span>
<span id="cb35-29"><a href="#cb35-29" aria-hidden="true" tabindex="-1"></a>        <span class="co"># Obtain the new state-values by taking the max over the action-values</span></span>
<span id="cb35-30"><a href="#cb35-30" aria-hidden="true" tabindex="-1"></a>        value <span class="op">=</span> np.<span class="bu">max</span>(q, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-31"><a href="#cb35-31" aria-hidden="true" tabindex="-1"></a>    <span class="co"># Define the policy accordingly</span></span>
<span id="cb35-32"><a href="#cb35-32" aria-hidden="true" tabindex="-1"></a>    pi <span class="op">=</span> np.argmax(q, axis<span class="op">=</span><span class="dv">1</span>)</span>
<span id="cb35-33"><a href="#cb35-33" aria-hidden="true" tabindex="-1"></a>    <span class="cf">return</span> value, pi</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
</div>
<p>Let’s check whether value iteration indeed obtains the same result as policy iteration in our <code>LakeEnv</code> problem:</p>
<div class="cell" data-execution_count="65">
<details>
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb36"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb36-1"><a href="#cb36-1" aria-hidden="true" tabindex="-1"></a>env <span class="op">=</span> gym.make(<span class="st">"FrozenLake4x4-v2"</span>)</span>
<span id="cb36-2"><a href="#cb36-2" aria-hidden="true" tabindex="-1"></a>value_PI, pi_PI <span class="op">=</span> policy_iteration(env.prob, env.reward)</span>
<span id="cb36-3"><a href="#cb36-3" aria-hidden="true" tabindex="-1"></a>value_VI, pi_VI <span class="op">=</span> value_iteration(env.prob, env.reward)</span>
<span id="cb36-4"><a href="#cb36-4" aria-hidden="true" tabindex="-1"></a></span>
<span id="cb36-5"><a href="#cb36-5" aria-hidden="true" tabindex="-1"></a>env.render(values<span class="op">=</span>value_VI, policy<span class="op">=</span>pi_VI)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display">
<p><img src="index_files/figure-html/cell-26-output-1.png" class="img-fluid"></p>
</div>
</div>
<div class="cell" data-execution_count="61">
<details open="">
<summary>Code</summary>
<div class="sourceCode cell-code" id="cb37"><pre class="sourceCode python code-with-copy"><code class="sourceCode python"><span id="cb37-1"><a href="#cb37-1" aria-hidden="true" tabindex="-1"></a>np.allclose(value_PI, value_VI)</span></code><button title="Copy to Clipboard" class="code-copy-button"><i class="bi"></i></button></pre></div>
</details>
<div class="cell-output cell-output-display" data-execution_count="61">
<pre><code>True</code></pre>
</div>
</div>
</section>
</section>
<section id="generalized-policy-iteration" class="level2">
<h2 class="anchored" data-anchor-id="generalized-policy-iteration">Generalized Policy Iteration</h2>
<p>In both algorithms presented above, the <strong>interaction of two competing processes</strong> is responsible for obtaining an optimal policy: estimating the value function of a policy (<strong>policy evaluation</strong>), and deriving a policy that is greedy with respect to the current value function (<strong>policy improvement</strong>). Both processes essentially create a moving target for each other and only stabilize when an optimal policy and the optimal value function have been found. This general pattern is called <strong>generalized policy iteration</strong> (GPI). Policy iteration and value iteration, like most methods in RL, are only two specific realizations of this pattern: while we wait for the convergence of policy evaluation in each step of policy iteration, we merely perform a single update in value iteration. There are many other possible approaches: for example, doing multiple iterations of policy evaluation in between each step of policy improvement can lead to faster convergence. Also, it is possible to asynchronously update some states several times before updating other values once. This can help in problems with large state spaces, especially if there are many states that are very unlikely to occur (though all states must be updated in the long run to guarantee convergence).</p>
<p>In the next post, we’ll see how we can solve the RL problem without complete knowledge of the environment.</p>


</section>

</main> <!-- /main -->
<script id="quarto-html-after-body" type="application/javascript">
window.document.addEventListener("DOMContentLoaded", function (event) {
  const toggleBodyColorMode = (bsSheetEl) => {
    const mode = bsSheetEl.getAttribute("data-mode");
    const bodyEl = window.document.querySelector("body");
    if (mode === "dark") {
      bodyEl.classList.add("quarto-dark");
      bodyEl.classList.remove("quarto-light");
    } else {
      bodyEl.classList.add("quarto-light");
      bodyEl.classList.remove("quarto-dark");
    }
  }
  const toggleBodyColorPrimary = () => {
    const bsSheetEl = window.document.querySelector("link#quarto-bootstrap");
    if (bsSheetEl) {
      toggleBodyColorMode(bsSheetEl);
    }
  }
  toggleBodyColorPrimary();  
  const disableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'prefetch';
    }
  }
  const enableStylesheet = (stylesheets) => {
    for (let i=0; i < stylesheets.length; i++) {
      const stylesheet = stylesheets[i];
      stylesheet.rel = 'stylesheet';
    }
  }
  const manageTransitions = (selector, allowTransitions) => {
    const els = window.document.querySelectorAll(selector);
    for (let i=0; i < els.length; i++) {
      const el = els[i];
      if (allowTransitions) {
        el.classList.remove('notransition');
      } else {
        el.classList.add('notransition');
      }
    }
  }
  const toggleColorMode = (alternate) => {
    // Switch the stylesheets
    const alternateStylesheets = window.document.querySelectorAll('link.quarto-color-scheme.quarto-color-alternate');
    manageTransitions('#quarto-margin-sidebar .nav-link', false);
    if (alternate) {
      enableStylesheet(alternateStylesheets);
      for (const sheetNode of alternateStylesheets) {
        if (sheetNode.id === "quarto-bootstrap") {
          toggleBodyColorMode(sheetNode);
        }
      }
    } else {
      disableStylesheet(alternateStylesheets);
      toggleBodyColorPrimary();
    }
    manageTransitions('#quarto-margin-sidebar .nav-link', true);
    // Switch the toggles
    const toggles = window.document.querySelectorAll('.quarto-color-scheme-toggle');
    for (let i=0; i < toggles.length; i++) {
      const toggle = toggles[i];
      if (toggle) {
        if (alternate) {
          toggle.classList.add("alternate");     
        } else {
          toggle.classList.remove("alternate");
        }
      }
    }
    // Hack to workaround the fact that safari doesn't
    // properly recolor the scrollbar when toggling (#1455)
    if (navigator.userAgent.indexOf('Safari') > 0 && navigator.userAgent.indexOf('Chrome') == -1) {
      manageTransitions("body", false);
      window.scrollTo(0, 1);
      setTimeout(() => {
        window.scrollTo(0, 0);
        manageTransitions("body", true);
      }, 40);  
    }
  }
  const isFileUrl = () => { 
    return window.location.protocol === 'file:';
  }
  const hasAlternateSentinel = () => {  
    let styleSentinel = getColorSchemeSentinel();
    if (styleSentinel !== null) {
      return styleSentinel === "alternate";
    } else {
      return false;
    }
  }
  const setStyleSentinel = (alternate) => {
    const value = alternate ? "alternate" : "default";
    if (!isFileUrl()) {
      window.localStorage.setItem("quarto-color-scheme", value);
    } else {
      localAlternateSentinel = value;
    }
  }
  const getColorSchemeSentinel = () => {
    if (!isFileUrl()) {
      const storageValue = window.localStorage.getItem("quarto-color-scheme");
      return storageValue != null ? storageValue : localAlternateSentinel;
    } else {
      return localAlternateSentinel;
    }
  }
  let localAlternateSentinel = 'default';
  // Dark / light mode switch
  window.quartoToggleColorScheme = () => {
    // Read the current dark / light value 
    let toAlternate = !hasAlternateSentinel();
    toggleColorMode(toAlternate);
    setStyleSentinel(toAlternate);
  };
  // Ensure there is a toggle, if there isn't float one in the top right
  if (window.document.querySelector('.quarto-color-scheme-toggle') === null) {
    const a = window.document.createElement('a');
    a.classList.add('top-right');
    a.classList.add('quarto-color-scheme-toggle');
    a.href = "";
    a.onclick = function() { try { window.quartoToggleColorScheme(); } catch {} return false; };
    const i = window.document.createElement("i");
    i.classList.add('bi');
    a.appendChild(i);
    window.document.body.appendChild(a);
  }
  // Switch to dark mode if need be
  if (hasAlternateSentinel()) {
    toggleColorMode(true);
  } else {
    toggleColorMode(false);
  }
  const icon = "";
  const anchorJS = new window.AnchorJS();
  anchorJS.options = {
    placement: 'right',
    icon: icon
  };
  anchorJS.add('.anchored');
  const isCodeAnnotation = (el) => {
    for (const clz of el.classList) {
      if (clz.startsWith('code-annotation-')) {                     
        return true;
      }
    }
    return false;
  }
  const clipboard = new window.ClipboardJS('.code-copy-button', {
    text: function(trigger) {
      const codeEl = trigger.previousElementSibling.cloneNode(true);
      for (const childEl of codeEl.children) {
        if (isCodeAnnotation(childEl)) {
          childEl.remove();
        }
      }
      return codeEl.innerText;
    }
  });
  clipboard.on('success', function(e) {
    // button target
    const button = e.trigger;
    // don't keep focus
    button.blur();
    // flash "checked"
    button.classList.add('code-copy-button-checked');
    var currentTitle = button.getAttribute("title");
    button.setAttribute("title", "Copied!");
    let tooltip;
    if (window.bootstrap) {
      button.setAttribute("data-bs-toggle", "tooltip");
      button.setAttribute("data-bs-placement", "left");
      button.setAttribute("data-bs-title", "Copied!");
      tooltip = new bootstrap.Tooltip(button, 
        { trigger: "manual", 
          customClass: "code-copy-button-tooltip",
          offset: [0, -8]});
      tooltip.show();    
    }
    setTimeout(function() {
      if (tooltip) {
        tooltip.hide();
        button.removeAttribute("data-bs-title");
        button.removeAttribute("data-bs-toggle");
        button.removeAttribute("data-bs-placement");
      }
      button.setAttribute("title", currentTitle);
      button.classList.remove('code-copy-button-checked');
    }, 1000);
    // clear code selection
    e.clearSelection();
  });
  function tippyHover(el, contentFn) {
    const config = {
      allowHTML: true,
      content: contentFn,
      maxWidth: 500,
      delay: 100,
      arrow: false,
      appendTo: function(el) {
          return el.parentElement;
      },
      interactive: true,
      interactiveBorder: 10,
      theme: 'quarto',
      placement: 'bottom-start'
    };
    window.tippy(el, config); 
  }
  const noterefs = window.document.querySelectorAll('a[role="doc-noteref"]');
  for (var i=0; i<noterefs.length; i++) {
    const ref = noterefs[i];
    tippyHover(ref, function() {
      // use id or data attribute instead here
      let href = ref.getAttribute('data-footnote-href') || ref.getAttribute('href');
      try { href = new URL(href).hash; } catch {}
      const id = href.replace(/^#\/?/, "");
      const note = window.document.getElementById(id);
      return note.innerHTML;
    });
  }
      let selectedAnnoteEl;
      const selectorForAnnotation = ( cell, annotation) => {
        let cellAttr = 'data-code-cell="' + cell + '"';
        let lineAttr = 'data-code-annotation="' +  annotation + '"';
        const selector = 'span[' + cellAttr + '][' + lineAttr + ']';
        return selector;
      }
      const selectCodeLines = (annoteEl) => {
        const doc = window.document;
        const targetCell = annoteEl.getAttribute("data-target-cell");
        const targetAnnotation = annoteEl.getAttribute("data-target-annotation");
        const annoteSpan = window.document.querySelector(selectorForAnnotation(targetCell, targetAnnotation));
        const lines = annoteSpan.getAttribute("data-code-lines").split(",");
        const lineIds = lines.map((line) => {
          return targetCell + "-" + line;
        })
        let top = null;
        let height = null;
        let parent = null;
        if (lineIds.length > 0) {
            //compute the position of the single el (top and bottom and make a div)
            const el = window.document.getElementById(lineIds[0]);
            top = el.offsetTop;
            height = el.offsetHeight;
            parent = el.parentElement.parentElement;
          if (lineIds.length > 1) {
            const lastEl = window.document.getElementById(lineIds[lineIds.length - 1]);
            const bottom = lastEl.offsetTop + lastEl.offsetHeight;
            height = bottom - top;
          }
          if (top !== null && height !== null && parent !== null) {
            // cook up a div (if necessary) and position it 
            let div = window.document.getElementById("code-annotation-line-highlight");
            if (div === null) {
              div = window.document.createElement("div");
              div.setAttribute("id", "code-annotation-line-highlight");
              div.style.position = 'absolute';
              parent.appendChild(div);
            }
            div.style.top = top - 2 + "px";
            div.style.height = height + 4 + "px";
            let gutterDiv = window.document.getElementById("code-annotation-line-highlight-gutter");
            if (gutterDiv === null) {
              gutterDiv = window.document.createElement("div");
              gutterDiv.setAttribute("id", "code-annotation-line-highlight-gutter");
              gutterDiv.style.position = 'absolute';
              const codeCell = window.document.getElementById(targetCell);
              const gutter = codeCell.querySelector('.code-annotation-gutter');
              gutter.appendChild(gutterDiv);
            }
            gutterDiv.style.top = top - 2 + "px";
            gutterDiv.style.height = height + 4 + "px";
          }
          selectedAnnoteEl = annoteEl;
        }
      };
      const unselectCodeLines = () => {
        const elementsIds = ["code-annotation-line-highlight", "code-annotation-line-highlight-gutter"];
        elementsIds.forEach((elId) => {
          const div = window.document.getElementById(elId);
          if (div) {
            div.remove();
          }
        });
        selectedAnnoteEl = undefined;
      };
      // Attach click handler to the DT
      const annoteDls = window.document.querySelectorAll('dt[data-target-cell]');
      for (const annoteDlNode of annoteDls) {
        annoteDlNode.addEventListener('click', (event) => {
          const clickedEl = event.target;
          if (clickedEl !== selectedAnnoteEl) {
            unselectCodeLines();
            const activeEl = window.document.querySelector('dt[data-target-cell].code-annotation-active');
            if (activeEl) {
              activeEl.classList.remove('code-annotation-active');
            }
            selectCodeLines(clickedEl);
            clickedEl.classList.add('code-annotation-active');
          } else {
            // Unselect the line
            unselectCodeLines();
            clickedEl.classList.remove('code-annotation-active');
          }
        });
      }
  const findCites = (el) => {
    const parentEl = el.parentElement;
    if (parentEl) {
      const cites = parentEl.dataset.cites;
      if (cites) {
        return {
          el,
          cites: cites.split(' ')
        };
      } else {
        return findCites(el.parentElement)
      }
    } else {
      return undefined;
    }
  };
  var bibliorefs = window.document.querySelectorAll('a[role="doc-biblioref"]');
  for (var i=0; i<bibliorefs.length; i++) {
    const ref = bibliorefs[i];
    const citeInfo = findCites(ref);
    if (citeInfo) {
      tippyHover(citeInfo.el, function() {
        var popup = window.document.createElement('div');
        citeInfo.cites.forEach(function(cite) {
          var citeDiv = window.document.createElement('div');
          citeDiv.classList.add('hanging-indent');
          citeDiv.classList.add('csl-entry');
          var biblioDiv = window.document.getElementById('ref-' + cite);
          if (biblioDiv) {
            citeDiv.innerHTML = biblioDiv.innerHTML;
          }
          popup.appendChild(citeDiv);
        });
        return popup.innerHTML;
      });
    }
  }
});
</script>
</div> <!-- /content -->



</body></html>